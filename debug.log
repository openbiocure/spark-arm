(venv) mohammad_shehab@EPAEDUBW001A ~/develop/spark git:(main*)% kubectl logs -n spark -l job-name=spark-test-job --tail=-1
Defaulted container "spark-test" out of: spark-test, copy-jars (init)
Defaulted container "spark-test" out of: spark-test, copy-jars (init)
=== Available JARs ===
-rw-r--r-- 1 root root    28650 May 11 15:40 /opt/spark/jars/delta-contribs_2.13-3.3.1.jar
-rw-r--r-- 1 root root    71286 May 11 15:40 /opt/spark/jars/delta-hive_2.13-3.3.1.jar
-rw-r--r-- 1 root root 11176695 May 11 15:40 /opt/spark/jars/delta-standalone_2.13-3.3.1.jar
-rw-r--r-- 1 root root    32229 May 11 15:40 /opt/spark/jars/delta-storage-3.3.1.jar

=== Classpath ===
SPARK_CLASSPATH: /opt/spark/jars/delta-contribs_2.13-3.3.1.jar,/opt/spark/jars/delta-hive_2.13-3.3.1.jar,/opt/spark/jars/delta-standalone_2.13-3.3.1.jar,/opt/spark/jars/delta-storage-3.3.1.jar
SPARK_DRIVER_CLASSPATH: /opt/spark/jars/delta-contribs_2.13-3.3.1.jar,/opt/spark/jars/delta-hive_2.13-3.3.1.jar,/opt/spark/jars/delta-standalone_2.13-3.3.1.jar,/opt/spark/jars/delta-storage-3.3.1.jar
SPARK_EXECUTOR_CLASSPATH: /opt/spark/jars/delta-contribs_2.13-3.3.1.jar,/opt/spark/jars/delta-hive_2.13-3.3.1.jar,/opt/spark/jars/delta-standalone_2.13-3.3.1.jar,/opt/spark/jars/delta-storage-3.3.1.jar

=== Java Options ===
SPARK_DRIVER_OPTS: -Dspark.driver.extraClassPath=/opt/spark/jars/delta-contribs_2.13-3.3.1.jar,/opt/spark/jars/delta-hive_2.13-3.3.1.jar,/opt/spark/jars/delta-standalone_2.13-3.3.1.jar,/opt/spark/jars/delta-storage-3.3.1.jar
SPARK_EXECUTOR_OPTS: -Dspark.executor.extraClassPath=/opt/spark/jars/delta-contribs_2.13-3.3.1.jar,/opt/spark/jars/delta-hive_2.13-3.3.1.jar,/opt/spark/jars/delta-standalone_2.13-3.3.1.jar,/opt/spark/jars/delta-storage-3.3.1.jar
DEBUG StatusLogger Using ShutdownCallbackRegistry class org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry
DEBUG StatusLogger Took 0.154783 seconds to load 258 plugins from jdk.internal.loader.ClassLoaders$AppClassLoader@5bc2b487
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'Converter' found 48 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger Starting LoggerContext[name=5bc2b487, org.apache.logging.log4j.core.LoggerContext@37883b97]...
DEBUG StatusLogger Reconfiguration started for context[name=5bc2b487] at URI null (org.apache.logging.log4j.core.LoggerContext@37883b97) with optional ClassLoader: null
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'ConfigurationFactory' found 6 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@7c83dc97
INFO StatusLogger Unable to locate file file:/opt/spark/conf/log4j2.xml, ignoring.
INFO StatusLogger Unable to locate file file:/opt/spark/conf/log4j2.xml, ignoring.
ERROR StatusLogger Reconfiguration failed: No configuration found for '5bc2b487' at 'null' in 'null'
DEBUG StatusLogger Shutdown hook enabled. Registering a new one.
DEBUG StatusLogger LoggerContext[name=5bc2b487, org.apache.logging.log4j.core.LoggerContext@37883b97] started OK.
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'Converter' found 48 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-2
DEBUG StatusLogger Starting LoggerContext[name=Default, org.apache.logging.log4j.core.LoggerContext@36d585c]...
DEBUG StatusLogger Reconfiguration started for context[name=Default] at URI null (org.apache.logging.log4j.core.LoggerContext@36d585c) with optional ClassLoader: null
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@7c83dc97
INFO StatusLogger Unable to locate file file:/opt/spark/conf/log4j2.xml, ignoring.
INFO StatusLogger Unable to locate file file:/opt/spark/conf/log4j2.xml, ignoring.
ERROR StatusLogger Reconfiguration failed: No configuration found for 'Default' at 'null' in 'null'
DEBUG StatusLogger Shutdown hook enabled. Registering a new one.
DEBUG StatusLogger LoggerContext[name=Default, org.apache.logging.log4j.core.LoggerContext@36d585c] started OK.
DEBUG StatusLogger Reconfiguration started for context[name=5bc2b487] at URI jar:file:/opt/spark/jars/spark-core_2.13-3.5.5.jar!/org/apache/spark/log4j2-defaults.properties (org.apache.logging.log4j.core.LoggerContext@37883b97) with optional ClassLoader: null
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@7c83dc97
DEBUG StatusLogger uri does not represent a local file: jar:file:/opt/spark/jars/spark-core_2.13-3.5.5.jar!/org/apache/spark/log4j2-defaults.properties
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger Apache Log4j Core 2.20 initializing configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@29e495ff
DEBUG StatusLogger PluginManager 'Core' found 134 plugins
DEBUG StatusLogger PluginManager 'Level' found 0 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
TRACE StatusLogger TypeConverterRegistry initializing.
DEBUG StatusLogger PluginManager 'TypeConverter' found 26 plugins
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="FATAL", levelAndRefs="null", name="org.apache.hadoop.hive.metastore.RetryingHMSHandler", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="ERROR", levelAndRefs="null", name="org.sparkproject.jetty.util.component.AbstractLifeCycle", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="INFO", levelAndRefs="null", name="org.apache.spark.repl.SparkIMain$exprTyper", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="INFO", levelAndRefs="null", name="org.apache.spark.repl.SparkILoop$SparkILoopInterpreter", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="ERROR", levelAndRefs="null", name="parquet.CorruptStatistics", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="WARN", levelAndRefs="null", name="org.apache.spark.repl.Main", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="WARN", levelAndRefs="null", name="org.sparkproject.jetty", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="ERROR", levelAndRefs="null", name="org.apache.hadoop.hive.ql.exec.FunctionRegistry", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="ERROR", levelAndRefs="null", name="org.apache.parquet.CorruptStatistics", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="console", level="null", Filter=null)
DEBUG StatusLogger Building Plugin[name=root, class=org.apache.logging.log4j.core.config.LoggerConfig$RootLogger].
DEBUG StatusLogger LoggerConfig$RootLogger$Builder(additivity="null", level="INFO", levelAndRefs="null", includeLocation="null", ={console}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=loggers, class=org.apache.logging.log4j.core.config.LoggersPlugin].
DEBUG StatusLogger createLoggers(={org.apache.hadoop.hive.metastore.RetryingHMSHandler, org.sparkproject.jetty.util.component.AbstractLifeCycle, org.apache.spark.repl.SparkIMain$exprTyper, org.apache.spark.repl.SparkILoop$SparkILoopInterpreter, parquet.CorruptStatistics, org.apache.spark.repl.Main, org.sparkproject.jetty, org.apache.hadoop.hive.ql.exec.FunctionRegistry, org.apache.parquet.CorruptStatistics, root})
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(pattern="%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n%ex", PatternSelector=null, Configuration, Replace=null, charset="null", alwaysWriteExceptions="null", disableAnsi="null", noConsoleNoAnsi="null", header="null", footer="null")
DEBUG StatusLogger PluginManager 'Converter' found 48 plugins
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.ConsoleAppender].
DEBUG StatusLogger ConsoleAppender$Builder(target="SYSTEM_ERR", follow="null", direct="null", bufferedIo="null", bufferSize="null", immediateFlush="null", ignoreExceptions="null", PatternLayout(%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n%ex), name="console", Configuration, Filter=null, ={})
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_ERR.false.false
DEBUG StatusLogger Building Plugin[name=appenders, class=org.apache.logging.log4j.core.config.AppendersPlugin].
DEBUG StatusLogger createAppenders(={console})
DEBUG StatusLogger Configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@29e495ff initialized
DEBUG StatusLogger Starting configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@29e495ff
DEBUG StatusLogger Started configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@29e495ff OK.
TRACE StatusLogger Stopping org.apache.logging.log4j.core.config.DefaultConfiguration@4d5d943d...
TRACE StatusLogger DefaultConfiguration notified 1 ReliabilityStrategies that config will be stopped.
TRACE StatusLogger DefaultConfiguration stopping root LoggerConfig.
TRACE StatusLogger DefaultConfiguration notifying ReliabilityStrategies that appenders will be stopped.
TRACE StatusLogger DefaultConfiguration stopping remaining Appenders.
DEBUG StatusLogger Shutting down OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger OutputStream closed
DEBUG StatusLogger Shut down OutputStreamManager SYSTEM_OUT.false.false-1, all resources released: true
DEBUG StatusLogger Appender DefaultConsole-1 stopped with status true
TRACE StatusLogger DefaultConfiguration stopped 1 remaining Appenders.
TRACE StatusLogger DefaultConfiguration cleaning Appenders from 1 LoggerConfigs.
DEBUG StatusLogger Stopped org.apache.logging.log4j.core.config.DefaultConfiguration@4d5d943d OK
TRACE StatusLogger Reregistering MBeans after reconfigure. Selector=org.apache.logging.log4j.core.selector.ClassLoaderContextSelector@747f281
TRACE StatusLogger Reregistering context (1/1): '5bc2b487' org.apache.logging.log4j.core.LoggerContext@37883b97
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=StatusLogger'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=ContextSelector'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=Appenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=AsyncAppenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=AsyncLoggerRingBuffer'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=*,subtype=RingBuffer'
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=StatusLogger
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=ContextSelector
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.sparkproject.jetty
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.sparkproject.jetty.util.component.AbstractLifeCycle
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.SparkIMain$exprTyper
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.SparkILoop$SparkILoopInterpreter
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.Main
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.parquet.CorruptStatistics
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.hadoop.hive.metastore.RetryingHMSHandler
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=parquet.CorruptStatistics
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.hadoop.hive.ql.exec.FunctionRegistry
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Appenders,name=console
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock supports precise timestamps.
TRACE StatusLogger Using DummyNanoClock for nanosecond timestamps.
DEBUG StatusLogger Reconfiguration complete for context[name=5bc2b487] at URI jar:file:/opt/spark/jars/spark-core_2.13-3.5.5.jar!/org/apache/spark/log4j2-defaults.properties (org.apache.logging.log4j.core.LoggerContext@37883b97) with optional ClassLoader: null
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.util.JavaUtils
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.conf.Configuration
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.conf.Configuration
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.util.ShutdownHookManager
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.util.Shell
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.commons.logging.impl.SLF4JLogFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.fs.FileSystem
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.security.UserGroupInformation
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.metrics2.impl.MetricsSystemImpl
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.metrics2.lib.Interns
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.metrics2.lib.MetricsSourceBuilder
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.metrics2.lib.MutableMetricsFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.security.SecurityUtil
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.security.authentication.util.KerberosName
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.security.HadoopKerberosName
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.security.Groups
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.util.NativeCodeLoader
25/05/11 15:40:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
DEBUG StatusLogger AsyncLogger.ThreadNameStrategy=UNCACHED (user specified null, default is UNCACHED)
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock supports precise timestamps.
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.util.PerformanceAdvisory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.security.ShellBasedUnixGroupsMapping
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.mapred.JobConf
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.mapreduce.lib.output.FileOutputFormat
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedPartitioner
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.fs.HarFileSystem
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.hdfs.DistributedFileSystem
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.hdfs.web.WebHdfsFileSystem
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.fs.Globber
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.fs.permission.FsPermission
DEBUG StatusLogger Reconfiguration started for context[name=5bc2b487] at URI jar:file:/opt/spark/jars/spark-core_2.13-3.5.5.jar!/org/apache/spark/log4j2-defaults.properties (org.apache.logging.log4j.core.LoggerContext@37883b97) with optional ClassLoader: null
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@7c83dc97
DEBUG StatusLogger uri does not represent a local file: jar:file:/opt/spark/jars/spark-core_2.13-3.5.5.jar!/org/apache/spark/log4j2-defaults.properties
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger Apache Log4j Core 2.20 initializing configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@18324f97
DEBUG StatusLogger PluginManager 'Core' found 134 plugins
DEBUG StatusLogger PluginManager 'Level' found 0 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="FATAL", levelAndRefs="null", name="org.apache.hadoop.hive.metastore.RetryingHMSHandler", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="ERROR", levelAndRefs="null", name="org.sparkproject.jetty.util.component.AbstractLifeCycle", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="INFO", levelAndRefs="null", name="org.apache.spark.repl.SparkIMain$exprTyper", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="INFO", levelAndRefs="null", name="org.apache.spark.repl.SparkILoop$SparkILoopInterpreter", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="ERROR", levelAndRefs="null", name="parquet.CorruptStatistics", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="WARN", levelAndRefs="null", name="org.apache.spark.repl.Main", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="WARN", levelAndRefs="null", name="org.sparkproject.jetty", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="ERROR", levelAndRefs="null", name="org.apache.hadoop.hive.ql.exec.FunctionRegistry", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="ERROR", levelAndRefs="null", name="org.apache.parquet.CorruptStatistics", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="console", level="null", Filter=null)
DEBUG StatusLogger Building Plugin[name=root, class=org.apache.logging.log4j.core.config.LoggerConfig$RootLogger].
DEBUG StatusLogger LoggerConfig$RootLogger$Builder(additivity="null", level="INFO", levelAndRefs="null", includeLocation="null", ={console}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=loggers, class=org.apache.logging.log4j.core.config.LoggersPlugin].
DEBUG StatusLogger createLoggers(={org.apache.hadoop.hive.metastore.RetryingHMSHandler, org.sparkproject.jetty.util.component.AbstractLifeCycle, org.apache.spark.repl.SparkIMain$exprTyper, org.apache.spark.repl.SparkILoop$SparkILoopInterpreter, parquet.CorruptStatistics, org.apache.spark.repl.Main, org.sparkproject.jetty, org.apache.hadoop.hive.ql.exec.FunctionRegistry, org.apache.parquet.CorruptStatistics, root})
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(pattern="%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n%ex", PatternSelector=null, Configuration, Replace=null, charset="null", alwaysWriteExceptions="null", disableAnsi="null", noConsoleNoAnsi="null", header="null", footer="null")
DEBUG StatusLogger PluginManager 'Converter' found 48 plugins
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.ConsoleAppender].
DEBUG StatusLogger ConsoleAppender$Builder(target="SYSTEM_ERR", follow="null", direct="null", bufferedIo="null", bufferSize="null", immediateFlush="null", ignoreExceptions="null", PatternLayout(%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n%ex), name="console", Configuration, Filter=null, ={})
DEBUG StatusLogger Building Plugin[name=appenders, class=org.apache.logging.log4j.core.config.AppendersPlugin].
DEBUG StatusLogger createAppenders(={console})
DEBUG StatusLogger Configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@18324f97 initialized
DEBUG StatusLogger Starting configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@18324f97
DEBUG StatusLogger Started configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@18324f97 OK.
TRACE StatusLogger Stopping org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@29e495ff...
TRACE StatusLogger PropertiesConfiguration notified 11 ReliabilityStrategies that config will be stopped.
TRACE StatusLogger PropertiesConfiguration stopping 10 LoggerConfigs.
TRACE StatusLogger PropertiesConfiguration stopping root LoggerConfig.
TRACE StatusLogger PropertiesConfiguration notifying ReliabilityStrategies that appenders will be stopped.
TRACE StatusLogger PropertiesConfiguration stopping remaining Appenders.
DEBUG StatusLogger Appender console stopped with status true
TRACE StatusLogger PropertiesConfiguration stopped 1 remaining Appenders.
TRACE StatusLogger PropertiesConfiguration cleaning Appenders from 11 LoggerConfigs.
DEBUG StatusLogger Stopped org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@29e495ff OK
TRACE StatusLogger Reregistering MBeans after reconfigure. Selector=org.apache.logging.log4j.core.selector.ClassLoaderContextSelector@747f281
TRACE StatusLogger Reregistering context (1/1): '5bc2b487' org.apache.logging.log4j.core.LoggerContext@37883b97
TRACE StatusLogger Unregistering 1 MBeans: [org.apache.logging.log4j2:type=5bc2b487]
TRACE StatusLogger Unregistering 1 MBeans: [org.apache.logging.log4j2:type=5bc2b487,component=StatusLogger]
TRACE StatusLogger Unregistering 1 MBeans: [org.apache.logging.log4j2:type=5bc2b487,component=ContextSelector]
TRACE StatusLogger Unregistering 10 MBeans: [org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.hadoop.hive.metastore.RetryingHMSHandler, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.hadoop.hive.ql.exec.FunctionRegistry, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.SparkIMain$exprTyper, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.Main, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.sparkproject.jetty.util.component.AbstractLifeCycle, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.parquet.CorruptStatistics, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=parquet.CorruptStatistics, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.sparkproject.jetty, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.SparkILoop$SparkILoopInterpreter]
TRACE StatusLogger Unregistering 1 MBeans: [org.apache.logging.log4j2:type=5bc2b487,component=Appenders,name=console]
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=AsyncAppenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=AsyncLoggerRingBuffer'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=*,subtype=RingBuffer'
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=StatusLogger
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=ContextSelector
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.sparkproject.jetty
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.sparkproject.jetty.util.component.AbstractLifeCycle
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.SparkIMain$exprTyper
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.SparkILoop$SparkILoopInterpreter
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.Main
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.parquet.CorruptStatistics
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.hadoop.hive.metastore.RetryingHMSHandler
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=parquet.CorruptStatistics
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.hadoop.hive.ql.exec.FunctionRegistry
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Appenders,name=console
TRACE StatusLogger Using DummyNanoClock for nanosecond timestamps.
DEBUG StatusLogger Reconfiguration complete for context[name=5bc2b487] at URI jar:file:/opt/spark/jars/spark-core_2.13-3.5.5.jar!/org/apache/spark/log4j2-defaults.properties (org.apache.logging.log4j.core.LoggerContext@37883b97) with optional ClassLoader: null
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.TransportContext
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.util.NettyLogger
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.protocol.MessageEncoder
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.protocol.MessageDecoder
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.server.RpcHandler$OneWayRpcCallback
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.client.TransportClientFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.server.TransportServer
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.shuffle.BlockStoreClient
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.client.TransportResponseHandler
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.client.TransportClient
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.server.ChunkFetchRequestHandler
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.server.TransportRequestHandler
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.server.TransportChannelHandler
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.server.OneForOneStreamManager
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
25/05/11 15:40:16 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.fs.FsUrlStreamHandlerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
25/05/11 15:40:17 WARN SharedState: Cannot qualify the warehouse path, leaving it unqualified.
java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.spark.sql.internal.SharedState$.qualifyWarehousePath(SharedState.scala:288)
	at org.apache.spark.sql.internal.SharedState.liftedTree1$1(SharedState.scala:80)
	at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:79)
	at org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)
	at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)
	at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.SparkSession.internalCreateDataFrame(SparkSession.scala:572)
	at org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:877)
	at org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:862)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
	... 36 more
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
Starting Spark Cluster Tests...

=== Verifying Delta Lake JARs ===
Found Delta JARs in /opt/spark/jars:
- delta-hive_2.13-3.3.1.jar
- delta-storage-3.3.1.jar
- delta-standalone_2.13-3.3.1.jar
- delta-contribs_2.13-3.3.1.jar

Classpath environment variables:
SPARK_CLASSPATH: /opt/spark/jars/delta-contribs_2.13-3.3.1.jar,/opt/spark/jars/delta-hive_2.13-3.3.1.jar,/opt/spark/jars/delta-standalone_2.13-3.3.1.jar,/opt/spark/jars/delta-storage-3.3.1.jar
SPARK_HOME: /opt/spark
SPARK_DAEMON_JAVA_OPTS: -Dlog4j.configuration=file:/opt/spark/conf/log4j2.xml -Dlog4j2.debug=true -Dspark.driver.extraClassPath=/opt/spark/jars/delta-*.jar:/opt/spark/jars/* -Dspark.executor.extraClassPath=/opt/spark/jars/delta-*.jar:/opt/spark/jars/* -Dspark.delta.autoCompact.enabled="true" -Dspark.delta.autoOptimize.autoCompact="true" -Dspark.delta.autoOptimize.optimizeWrite="true" -Dspark.delta.concurrent.writes.enabled="true" -Dspark.delta.concurrent.writes.maxConcurrentWrites="10" -Dspark.delta.logStore.class="org.apache.spark.sql.delta.storage.S3SingleDriverLogStore" -Dspark.delta.merge.repartitionBeforeWrite="true" -Dspark.delta.optimizeWrite.enabled="true" -Dspark.delta.optimizeWrite.numShuffleBlocks="200" -Dspark.delta.optimizeWrite.targetFileSize="128m" -Dspark.delta.schema.autoMerge.enabled="true" -Dspark.delta.storage.s3.impl="org.apache.hadoop.fs.s3a.S3AFileSystem" -Dspark.delta.storage.s3a.impl="org.apache.hadoop.fs.s3a.S3AFileSystem" -Dspark.delta.timeTravel.enabled="true" -Dspark.delta.timeTravel.retentionPeriod="168h" -Dspark.delta.warehouse.dir="s3a://${MINIO_BUCKET}/delta" -Dspark.sql.catalog.spark_catalog="org.apache.spark.sql.delta.catalog.DeltaCatalog" -Dspark.sql.extensions="io.delta.sql.DeltaSparkSessionExtension"

Using Delta JARs: /opt/spark/jars/delta-hive_2.13-3.3.1.jar,/opt/spark/jars/delta-storage-3.3.1.jar,/opt/spark/jars/delta-standalone_2.13-3.3.1.jar,/opt/spark/jars/delta-contribs_2.13-3.3.1.jar

Setting up Spark session with debug logging...

Running Basic Spark test...

=== Testing Basic Spark Functionality ===
Created DataFrame:
 Basic Spark test failed: An error occurred while calling o106.showString.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:454)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:242)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:169)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:165)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 63 more


Running MinIO test...

=== Testing MinIO Connectivity ===
Writing test data to s3a:///test/minio_test
 MinIO test failed: An error occurred while calling o128.parquet.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
	... 25 more


Running Hive test...

=== Testing Hive Metastore ===
Creating Hive table: test_hive_table
 Hive test failed: An error occurred while calling o150.saveAsTable.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:454)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:242)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:570)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 20 more


Running Delta Lake test...

=== Testing Delta Lake ===
Creating Delta table at s3a:///delta/test_delta
 Delta Lake test failed: An error occurred while calling o173.save.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:221)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 16 more


=== Test Summary ===
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
Basic Spark: FAILED: An error occurred while calling o106.showString.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:454)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:242)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:169)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:165)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 63 more

MinIO: FAILED: An error occurred while calling o128.parquet.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
	... 25 more

Hive: FAILED: An error occurred while calling o150.saveAsTable.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:454)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:242)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:570)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 20 more

Delta Lake: FAILED: An error occurred while calling o173.save.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:221)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 16 more

=== Available JARs ===
-rw-r--r-- 1 root root    28650 May 11 15:39 /opt/spark/jars/delta-contribs_2.13-3.3.1.jar
-rw-r--r-- 1 root root    71286 May 11 15:39 /opt/spark/jars/delta-hive_2.13-3.3.1.jar
-rw-r--r-- 1 root root 11176695 May 11 15:39 /opt/spark/jars/delta-standalone_2.13-3.3.1.jar
-rw-r--r-- 1 root root    32229 May 11 15:39 /opt/spark/jars/delta-storage-3.3.1.jar

=== Classpath ===
SPARK_CLASSPATH: /opt/spark/jars/delta-contribs_2.13-3.3.1.jar,/opt/spark/jars/delta-hive_2.13-3.3.1.jar,/opt/spark/jars/delta-standalone_2.13-3.3.1.jar,/opt/spark/jars/delta-storage-3.3.1.jar
SPARK_DRIVER_CLASSPATH: /opt/spark/jars/delta-contribs_2.13-3.3.1.jar,/opt/spark/jars/delta-hive_2.13-3.3.1.jar,/opt/spark/jars/delta-standalone_2.13-3.3.1.jar,/opt/spark/jars/delta-storage-3.3.1.jar
SPARK_EXECUTOR_CLASSPATH: /opt/spark/jars/delta-contribs_2.13-3.3.1.jar,/opt/spark/jars/delta-hive_2.13-3.3.1.jar,/opt/spark/jars/delta-standalone_2.13-3.3.1.jar,/opt/spark/jars/delta-storage-3.3.1.jar

=== Java Options ===
SPARK_DRIVER_OPTS: -Dspark.driver.extraClassPath=/opt/spark/jars/delta-contribs_2.13-3.3.1.jar,/opt/spark/jars/delta-hive_2.13-3.3.1.jar,/opt/spark/jars/delta-standalone_2.13-3.3.1.jar,/opt/spark/jars/delta-storage-3.3.1.jar
SPARK_EXECUTOR_OPTS: -Dspark.executor.extraClassPath=/opt/spark/jars/delta-contribs_2.13-3.3.1.jar,/opt/spark/jars/delta-hive_2.13-3.3.1.jar,/opt/spark/jars/delta-standalone_2.13-3.3.1.jar,/opt/spark/jars/delta-storage-3.3.1.jar
DEBUG StatusLogger Using ShutdownCallbackRegistry class org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry
DEBUG StatusLogger Took 0.133096 seconds to load 258 plugins from jdk.internal.loader.ClassLoaders$AppClassLoader@5bc2b487
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'Converter' found 48 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger Starting LoggerContext[name=5bc2b487, org.apache.logging.log4j.core.LoggerContext@37883b97]...
DEBUG StatusLogger Reconfiguration started for context[name=5bc2b487] at URI null (org.apache.logging.log4j.core.LoggerContext@37883b97) with optional ClassLoader: null
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'ConfigurationFactory' found 6 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@7c83dc97
INFO StatusLogger Unable to locate file file:/opt/spark/conf/log4j2.xml, ignoring.
INFO StatusLogger Unable to locate file file:/opt/spark/conf/log4j2.xml, ignoring.
ERROR StatusLogger Reconfiguration failed: No configuration found for '5bc2b487' at 'null' in 'null'
DEBUG StatusLogger Shutdown hook enabled. Registering a new one.
DEBUG StatusLogger LoggerContext[name=5bc2b487, org.apache.logging.log4j.core.LoggerContext@37883b97] started OK.
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger PluginManager 'Converter' found 48 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-2
DEBUG StatusLogger Starting LoggerContext[name=Default, org.apache.logging.log4j.core.LoggerContext@36d585c]...
DEBUG StatusLogger Reconfiguration started for context[name=Default] at URI null (org.apache.logging.log4j.core.LoggerContext@36d585c) with optional ClassLoader: null
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@7c83dc97
INFO StatusLogger Unable to locate file file:/opt/spark/conf/log4j2.xml, ignoring.
INFO StatusLogger Unable to locate file file:/opt/spark/conf/log4j2.xml, ignoring.
ERROR StatusLogger Reconfiguration failed: No configuration found for 'Default' at 'null' in 'null'
DEBUG StatusLogger Shutdown hook enabled. Registering a new one.
DEBUG StatusLogger LoggerContext[name=Default, org.apache.logging.log4j.core.LoggerContext@36d585c] started OK.
DEBUG StatusLogger Reconfiguration started for context[name=5bc2b487] at URI jar:file:/opt/spark/jars/spark-core_2.13-3.5.5.jar!/org/apache/spark/log4j2-defaults.properties (org.apache.logging.log4j.core.LoggerContext@37883b97) with optional ClassLoader: null
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@7c83dc97
DEBUG StatusLogger uri does not represent a local file: jar:file:/opt/spark/jars/spark-core_2.13-3.5.5.jar!/org/apache/spark/log4j2-defaults.properties
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger Apache Log4j Core 2.20 initializing configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@29e495ff
DEBUG StatusLogger PluginManager 'Core' found 134 plugins
DEBUG StatusLogger PluginManager 'Level' found 0 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
TRACE StatusLogger TypeConverterRegistry initializing.
DEBUG StatusLogger PluginManager 'TypeConverter' found 26 plugins
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="FATAL", levelAndRefs="null", name="org.apache.hadoop.hive.metastore.RetryingHMSHandler", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="ERROR", levelAndRefs="null", name="org.sparkproject.jetty.util.component.AbstractLifeCycle", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="INFO", levelAndRefs="null", name="org.apache.spark.repl.SparkIMain$exprTyper", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="INFO", levelAndRefs="null", name="org.apache.spark.repl.SparkILoop$SparkILoopInterpreter", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="ERROR", levelAndRefs="null", name="parquet.CorruptStatistics", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="WARN", levelAndRefs="null", name="org.apache.spark.repl.Main", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="WARN", levelAndRefs="null", name="org.sparkproject.jetty", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="ERROR", levelAndRefs="null", name="org.apache.hadoop.hive.ql.exec.FunctionRegistry", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="ERROR", levelAndRefs="null", name="org.apache.parquet.CorruptStatistics", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="console", level="null", Filter=null)
DEBUG StatusLogger Building Plugin[name=root, class=org.apache.logging.log4j.core.config.LoggerConfig$RootLogger].
DEBUG StatusLogger LoggerConfig$RootLogger$Builder(additivity="null", level="INFO", levelAndRefs="null", includeLocation="null", ={console}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=loggers, class=org.apache.logging.log4j.core.config.LoggersPlugin].
DEBUG StatusLogger createLoggers(={org.apache.hadoop.hive.metastore.RetryingHMSHandler, org.sparkproject.jetty.util.component.AbstractLifeCycle, org.apache.spark.repl.SparkIMain$exprTyper, org.apache.spark.repl.SparkILoop$SparkILoopInterpreter, parquet.CorruptStatistics, org.apache.spark.repl.Main, org.sparkproject.jetty, org.apache.hadoop.hive.ql.exec.FunctionRegistry, org.apache.parquet.CorruptStatistics, root})
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(pattern="%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n%ex", PatternSelector=null, Configuration, Replace=null, charset="null", alwaysWriteExceptions="null", disableAnsi="null", noConsoleNoAnsi="null", header="null", footer="null")
DEBUG StatusLogger PluginManager 'Converter' found 48 plugins
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.ConsoleAppender].
DEBUG StatusLogger ConsoleAppender$Builder(target="SYSTEM_ERR", follow="null", direct="null", bufferedIo="null", bufferSize="null", immediateFlush="null", ignoreExceptions="null", PatternLayout(%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n%ex), name="console", Configuration, Filter=null, ={})
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_ERR.false.false
DEBUG StatusLogger Building Plugin[name=appenders, class=org.apache.logging.log4j.core.config.AppendersPlugin].
DEBUG StatusLogger createAppenders(={console})
DEBUG StatusLogger Configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@29e495ff initialized
DEBUG StatusLogger Starting configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@29e495ff
DEBUG StatusLogger Started configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@29e495ff OK.
TRACE StatusLogger Stopping org.apache.logging.log4j.core.config.DefaultConfiguration@4d5d943d...
TRACE StatusLogger DefaultConfiguration notified 1 ReliabilityStrategies that config will be stopped.
TRACE StatusLogger DefaultConfiguration stopping root LoggerConfig.
TRACE StatusLogger DefaultConfiguration notifying ReliabilityStrategies that appenders will be stopped.
TRACE StatusLogger DefaultConfiguration stopping remaining Appenders.
DEBUG StatusLogger Shutting down OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger OutputStream closed
DEBUG StatusLogger Shut down OutputStreamManager SYSTEM_OUT.false.false-1, all resources released: true
DEBUG StatusLogger Appender DefaultConsole-1 stopped with status true
TRACE StatusLogger DefaultConfiguration stopped 1 remaining Appenders.
TRACE StatusLogger DefaultConfiguration cleaning Appenders from 1 LoggerConfigs.
DEBUG StatusLogger Stopped org.apache.logging.log4j.core.config.DefaultConfiguration@4d5d943d OK
TRACE StatusLogger Reregistering MBeans after reconfigure. Selector=org.apache.logging.log4j.core.selector.ClassLoaderContextSelector@747f281
TRACE StatusLogger Reregistering context (1/1): '5bc2b487' org.apache.logging.log4j.core.LoggerContext@37883b97
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=StatusLogger'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=ContextSelector'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=Appenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=AsyncAppenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=AsyncLoggerRingBuffer'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=*,subtype=RingBuffer'
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=StatusLogger
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=ContextSelector
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.sparkproject.jetty
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.sparkproject.jetty.util.component.AbstractLifeCycle
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.SparkIMain$exprTyper
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.SparkILoop$SparkILoopInterpreter
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.Main
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.parquet.CorruptStatistics
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.hadoop.hive.metastore.RetryingHMSHandler
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=parquet.CorruptStatistics
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.hadoop.hive.ql.exec.FunctionRegistry
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Appenders,name=console
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock supports precise timestamps.
TRACE StatusLogger Using DummyNanoClock for nanosecond timestamps.
DEBUG StatusLogger Reconfiguration complete for context[name=5bc2b487] at URI jar:file:/opt/spark/jars/spark-core_2.13-3.5.5.jar!/org/apache/spark/log4j2-defaults.properties (org.apache.logging.log4j.core.LoggerContext@37883b97) with optional ClassLoader: null
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.util.JavaUtils
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.conf.Configuration
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.conf.Configuration
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.util.ShutdownHookManager
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.util.Shell
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.commons.logging.impl.SLF4JLogFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.fs.FileSystem
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.security.UserGroupInformation
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.metrics2.impl.MetricsSystemImpl
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.metrics2.lib.Interns
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.metrics2.lib.MetricsSourceBuilder
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.metrics2.lib.MutableMetricsFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.security.SecurityUtil
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.security.authentication.util.KerberosName
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.security.HadoopKerberosName
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.security.Groups
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.util.NativeCodeLoader
25/05/11 15:39:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
DEBUG StatusLogger AsyncLogger.ThreadNameStrategy=UNCACHED (user specified null, default is UNCACHED)
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock supports precise timestamps.
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.util.PerformanceAdvisory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.security.ShellBasedUnixGroupsMapping
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.mapred.JobConf
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.mapreduce.lib.output.FileOutputFormat
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedPartitioner
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.fs.HarFileSystem
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.hdfs.DistributedFileSystem
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.hdfs.web.WebHdfsFileSystem
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.fs.Globber
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.fs.permission.FsPermission
DEBUG StatusLogger Reconfiguration started for context[name=5bc2b487] at URI jar:file:/opt/spark/jars/spark-core_2.13-3.5.5.jar!/org/apache/spark/log4j2-defaults.properties (org.apache.logging.log4j.core.LoggerContext@37883b97) with optional ClassLoader: null
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@7c83dc97
DEBUG StatusLogger uri does not represent a local file: jar:file:/opt/spark/jars/spark-core_2.13-3.5.5.jar!/org/apache/spark/log4j2-defaults.properties
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger Apache Log4j Core 2.20 initializing configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@18324f97
DEBUG StatusLogger PluginManager 'Core' found 134 plugins
DEBUG StatusLogger PluginManager 'Level' found 0 plugins
DEBUG StatusLogger PluginManager 'Lookup' found 16 plugins
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="FATAL", levelAndRefs="null", name="org.apache.hadoop.hive.metastore.RetryingHMSHandler", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="ERROR", levelAndRefs="null", name="org.sparkproject.jetty.util.component.AbstractLifeCycle", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="INFO", levelAndRefs="null", name="org.apache.spark.repl.SparkIMain$exprTyper", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="INFO", levelAndRefs="null", name="org.apache.spark.repl.SparkILoop$SparkILoopInterpreter", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="ERROR", levelAndRefs="null", name="parquet.CorruptStatistics", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="WARN", levelAndRefs="null", name="org.apache.spark.repl.Main", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="WARN", levelAndRefs="null", name="org.sparkproject.jetty", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="ERROR", levelAndRefs="null", name="org.apache.hadoop.hive.ql.exec.FunctionRegistry", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger LoggerConfig$Builder(additivity="null", level="ERROR", levelAndRefs="null", name="org.apache.parquet.CorruptStatistics", includeLocation="null", ={}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="console", level="null", Filter=null)
DEBUG StatusLogger Building Plugin[name=root, class=org.apache.logging.log4j.core.config.LoggerConfig$RootLogger].
DEBUG StatusLogger LoggerConfig$RootLogger$Builder(additivity="null", level="INFO", levelAndRefs="null", includeLocation="null", ={console}, ={}, Configuration, Filter=null)
DEBUG StatusLogger Building Plugin[name=loggers, class=org.apache.logging.log4j.core.config.LoggersPlugin].
DEBUG StatusLogger createLoggers(={org.apache.hadoop.hive.metastore.RetryingHMSHandler, org.sparkproject.jetty.util.component.AbstractLifeCycle, org.apache.spark.repl.SparkIMain$exprTyper, org.apache.spark.repl.SparkILoop$SparkILoopInterpreter, parquet.CorruptStatistics, org.apache.spark.repl.Main, org.sparkproject.jetty, org.apache.hadoop.hive.ql.exec.FunctionRegistry, org.apache.parquet.CorruptStatistics, root})
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(pattern="%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n%ex", PatternSelector=null, Configuration, Replace=null, charset="null", alwaysWriteExceptions="null", disableAnsi="null", noConsoleNoAnsi="null", header="null", footer="null")
DEBUG StatusLogger PluginManager 'Converter' found 48 plugins
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.ConsoleAppender].
DEBUG StatusLogger ConsoleAppender$Builder(target="SYSTEM_ERR", follow="null", direct="null", bufferedIo="null", bufferSize="null", immediateFlush="null", ignoreExceptions="null", PatternLayout(%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n%ex), name="console", Configuration, Filter=null, ={})
DEBUG StatusLogger Building Plugin[name=appenders, class=org.apache.logging.log4j.core.config.AppendersPlugin].
DEBUG StatusLogger createAppenders(={console})
DEBUG StatusLogger Configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@18324f97 initialized
DEBUG StatusLogger Starting configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@18324f97
DEBUG StatusLogger Started configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@18324f97 OK.
TRACE StatusLogger Stopping org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@29e495ff...
TRACE StatusLogger PropertiesConfiguration notified 11 ReliabilityStrategies that config will be stopped.
TRACE StatusLogger PropertiesConfiguration stopping 10 LoggerConfigs.
TRACE StatusLogger PropertiesConfiguration stopping root LoggerConfig.
TRACE StatusLogger PropertiesConfiguration notifying ReliabilityStrategies that appenders will be stopped.
TRACE StatusLogger PropertiesConfiguration stopping remaining Appenders.
DEBUG StatusLogger Appender console stopped with status true
TRACE StatusLogger PropertiesConfiguration stopped 1 remaining Appenders.
TRACE StatusLogger PropertiesConfiguration cleaning Appenders from 11 LoggerConfigs.
DEBUG StatusLogger Stopped org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@29e495ff OK
TRACE StatusLogger Reregistering MBeans after reconfigure. Selector=org.apache.logging.log4j.core.selector.ClassLoaderContextSelector@747f281
TRACE StatusLogger Reregistering context (1/1): '5bc2b487' org.apache.logging.log4j.core.LoggerContext@37883b97
TRACE StatusLogger Unregistering 1 MBeans: [org.apache.logging.log4j2:type=5bc2b487]
TRACE StatusLogger Unregistering 1 MBeans: [org.apache.logging.log4j2:type=5bc2b487,component=StatusLogger]
TRACE StatusLogger Unregistering 1 MBeans: [org.apache.logging.log4j2:type=5bc2b487,component=ContextSelector]
TRACE StatusLogger Unregistering 10 MBeans: [org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.hadoop.hive.metastore.RetryingHMSHandler, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.hadoop.hive.ql.exec.FunctionRegistry, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.SparkIMain$exprTyper, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.Main, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.sparkproject.jetty.util.component.AbstractLifeCycle, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.parquet.CorruptStatistics, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=parquet.CorruptStatistics, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.sparkproject.jetty, org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.SparkILoop$SparkILoopInterpreter]
TRACE StatusLogger Unregistering 1 MBeans: [org.apache.logging.log4j2:type=5bc2b487,component=Appenders,name=console]
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=AsyncAppenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=AsyncLoggerRingBuffer'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=*,subtype=RingBuffer'
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=StatusLogger
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=ContextSelector
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.sparkproject.jetty
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.sparkproject.jetty.util.component.AbstractLifeCycle
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.SparkIMain$exprTyper
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.SparkILoop$SparkILoopInterpreter
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.spark.repl.Main
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.parquet.CorruptStatistics
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.hadoop.hive.metastore.RetryingHMSHandler
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=parquet.CorruptStatistics
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Loggers,name=org.apache.hadoop.hive.ql.exec.FunctionRegistry
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=5bc2b487,component=Appenders,name=console
TRACE StatusLogger Using DummyNanoClock for nanosecond timestamps.
DEBUG StatusLogger Reconfiguration complete for context[name=5bc2b487] at URI jar:file:/opt/spark/jars/spark-core_2.13-3.5.5.jar!/org/apache/spark/log4j2-defaults.properties (org.apache.logging.log4j.core.LoggerContext@37883b97) with optional ClassLoader: null
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.TransportContext
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.util.NettyLogger
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.protocol.MessageEncoder
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.protocol.MessageDecoder
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.server.RpcHandler$OneWayRpcCallback
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.client.TransportClientFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.server.TransportServer
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.shuffle.BlockStoreClient
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.client.TransportResponseHandler
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.client.TransportClient
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.server.ChunkFetchRequestHandler
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.server.TransportRequestHandler
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.server.TransportChannelHandler
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class io.netty.util.internal.logging.Slf4JLoggerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.spark.network.server.OneForOneStreamManager
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
25/05/11 15:39:49 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.apache.hadoop.fs.FsUrlStreamHandlerFactory
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
25/05/11 15:39:50 WARN SharedState: Cannot qualify the warehouse path, leaving it unqualified.
java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.spark.sql.internal.SharedState$.qualifyWarehousePath(SharedState.scala:288)
	at org.apache.spark.sql.internal.SharedState.liftedTree1$1(SharedState.scala:80)
	at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:79)
	at org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)
	at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)
	at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.SparkSession.internalCreateDataFrame(SparkSession.scala:572)
	at org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:877)
	at org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:862)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
	... 36 more
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
Starting Spark Cluster Tests...

=== Verifying Delta Lake JARs ===
Found Delta JARs in /opt/spark/jars:
- delta-hive_2.13-3.3.1.jar
- delta-storage-3.3.1.jar
- delta-standalone_2.13-3.3.1.jar
- delta-contribs_2.13-3.3.1.jar

Classpath environment variables:
SPARK_CLASSPATH: /opt/spark/jars/delta-contribs_2.13-3.3.1.jar,/opt/spark/jars/delta-hive_2.13-3.3.1.jar,/opt/spark/jars/delta-standalone_2.13-3.3.1.jar,/opt/spark/jars/delta-storage-3.3.1.jar
SPARK_HOME: /opt/spark
SPARK_DAEMON_JAVA_OPTS: -Dlog4j.configuration=file:/opt/spark/conf/log4j2.xml -Dlog4j2.debug=true -Dspark.driver.extraClassPath=/opt/spark/jars/delta-*.jar:/opt/spark/jars/* -Dspark.executor.extraClassPath=/opt/spark/jars/delta-*.jar:/opt/spark/jars/* -Dspark.delta.autoCompact.enabled="true" -Dspark.delta.autoOptimize.autoCompact="true" -Dspark.delta.autoOptimize.optimizeWrite="true" -Dspark.delta.concurrent.writes.enabled="true" -Dspark.delta.concurrent.writes.maxConcurrentWrites="10" -Dspark.delta.logStore.class="org.apache.spark.sql.delta.storage.S3SingleDriverLogStore" -Dspark.delta.merge.repartitionBeforeWrite="true" -Dspark.delta.optimizeWrite.enabled="true" -Dspark.delta.optimizeWrite.numShuffleBlocks="200" -Dspark.delta.optimizeWrite.targetFileSize="128m" -Dspark.delta.schema.autoMerge.enabled="true" -Dspark.delta.storage.s3.impl="org.apache.hadoop.fs.s3a.S3AFileSystem" -Dspark.delta.storage.s3a.impl="org.apache.hadoop.fs.s3a.S3AFileSystem" -Dspark.delta.timeTravel.enabled="true" -Dspark.delta.timeTravel.retentionPeriod="168h" -Dspark.delta.warehouse.dir="s3a://${MINIO_BUCKET}/delta" -Dspark.sql.catalog.spark_catalog="org.apache.spark.sql.delta.catalog.DeltaCatalog" -Dspark.sql.extensions="io.delta.sql.DeltaSparkSessionExtension"

Using Delta JARs: /opt/spark/jars/delta-hive_2.13-3.3.1.jar,/opt/spark/jars/delta-storage-3.3.1.jar,/opt/spark/jars/delta-standalone_2.13-3.3.1.jar,/opt/spark/jars/delta-contribs_2.13-3.3.1.jar

Setting up Spark session with debug logging...

Running Basic Spark test...

=== Testing Basic Spark Functionality ===
Created DataFrame:
 Basic Spark test failed: An error occurred while calling o106.showString.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:454)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:242)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:169)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:165)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 63 more


Running MinIO test...

=== Testing MinIO Connectivity ===
Writing test data to s3a:///test/minio_test
 MinIO test failed: An error occurred while calling o128.parquet.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
	... 25 more


Running Hive test...

=== Testing Hive Metastore ===
Creating Hive table: test_hive_table
 Hive test failed: An error occurred while calling o150.saveAsTable.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:454)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:242)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:570)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 20 more


Running Delta Lake test...

=== Testing Delta Lake ===
Creating Delta table at s3a:///delta/test_delta
 Delta Lake test failed: An error occurred while calling o173.save.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:221)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 16 more


=== Test Summary ===
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor class org.sparkproject.jetty.util.log.Slf4jLog
TRACE StatusLogger Log4jLoggerFactory.getContext() found anchor interface org.apache.spark.internal.Logging
Basic Spark: FAILED: An error occurred while calling o106.showString.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:454)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:242)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:169)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:165)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 63 more

MinIO: FAILED: An error occurred while calling o128.parquet.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
	... 25 more

Hive: FAILED: An error occurred while calling o150.saveAsTable.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:454)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:242)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:570)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 20 more

Delta Lake: FAILED: An error occurred while calling o173.save.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:221)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 16 more

(venv) mohammad_shehab@EPAEDUBW001A ~/develop/spark git:(main*)%