services:
  spark-master:
    image: ${SPARK_IMAGE_NAME:-ghcr.io/openbiocure/spark-arm}:${TAG:-v0.6.2}
    platform: linux/arm64
    container_name: spark-test
    hostname: spark-test
    user: "1000:1000"  # Use explicit UID/GID
    env_file:
      - debug.env
    ports:
      - "7077:7077"  # Spark master
      - "8080:8080"  # Spark master web UI
    environment:
      - SPARK_NODE_TYPE=master
      - SPARK_MASTER_HOST=spark-test
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_WORKER_DIR=/opt/spark/work
      - SPARK_LOCAL_DIRS=/opt/spark/tmp
      - SPARK_DRIVER_DIR=/opt/spark/tmp
      - HADOOP_USER_NAME=spark
      - SPARK_USER=spark
    volumes:
      - ./mount/test-mount/logs:/opt/spark/logs
      - ./mount/test-mount/logs/events:/opt/spark/logs/events
      - ./mount/test-mount/tmp:/opt/spark/tmp
      - ./mount/test-mount/work:/opt/spark/work
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master", "--host", "spark-test", "--port", "7077", "--webui-port", "8080"]
    networks:
      - spark-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  spark-worker:
    image: ${SPARK_IMAGE_NAME:-ghcr.io/openbiocure/spark-arm}:${TAG:-v0.6.2}
    platform: linux/arm64
    container_name: spark-test-worker
    hostname: spark-test-worker
    user: "1000:1000"  # Use explicit UID/GID
    env_file:
      - debug.env
    ports:
      - "8082:8082"  # Spark worker web UI
    environment:
      - SPARK_NODE_TYPE=worker
      - SPARK_MASTER_URL=spark://spark-test:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_LOCAL_DIRS=/opt/spark/tmp
      - SPARK_WORKER_DIR=/opt/spark/work
      - SPARK_DRIVER_DIR=/opt/spark/tmp
      - SPARK_HOME=/opt/spark
      - SPARK_CLASSPATH=/opt/spark/jars/*
      - SPARK_SQL_CATALOG_IMPLEMENTATION=hive
      - SPARK_JDBC_DRIVER=org.postgresql.Driver
      - SPARK_JDBC_URL=jdbc:postgresql://172.16.14.112:5432/hive
      - SPARK_JDBC_USER=hive
      - SPARK_JDBC_PASSWORD=hive
      - SPARK_DIST_CLASSPATH=/opt/spark/jars/*
      - SPARK_RESOURCES_DIR=/opt/spark
      - SPARK_CONF_DIR=/opt/spark/conf
      - AWS_ENDPOINT_URL=${AWS_ENDPOINT_URL}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - SPARK_DAEMON_JAVA_OPTS=-Djava.net.preferIPv4Stack=true -Dspark.worker.bindAddress=0.0.0.0 -Dspark.worker.webui.bindAddress=0.0.0.0
    volumes:
      - ./mount/test-mount/logs:/opt/spark/logs
      - ./mount/test-mount/logs/events:/opt/spark/logs/events
      - ./mount/test-mount/tmp:/opt/spark/tmp
      - ./mount/test-mount/work:/opt/spark/work
    command: /opt/spark/scripts/start-worker.sh
    networks:
      - spark-net
    depends_on:
      spark-master:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "ps aux | grep -v grep | grep -q 'org.apache.spark.deploy.worker.Worker'"]
      interval: 5s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1.5G
        reservations:
          memory: 512M

  spark-client:
    image: ${SPARK_IMAGE_NAME:-ghcr.io/openbiocure/spark-arm}:${TAG:-v0.6.2}
    platform: linux/arm64
    container_name: spark-test-client
    hostname: spark-test-client
    user: "1000:1000"  # Use explicit UID/GID
    env_file:
      - debug.env
    environment:
      - SPARK_MASTER_URL=spark://spark-test:7077
      - SPARK_MASTER=spark://spark-test:7077
      - SPARK_HOME=/opt/spark
      - SPARK_CLASSPATH=/opt/spark/jars/*
      - SPARK_LOCAL_DIRS=/opt/spark/tmp
      - SPARK_WORKER_DIR=/opt/spark/work
      - SPARK_DRIVER_DIR=/opt/spark/tmp
    volumes:
      - ./mount/test-mount/logs:/opt/spark/logs
      - ./mount/test-mount/logs/events:/opt/spark/logs/events
      - ./mount/client/tmp:/opt/spark/tmp
      - ./mount/client/work:/opt/spark/work
    networks:
      - spark-net
    depends_on:
      spark-master:
        condition: service_healthy
      spark-worker:
        condition: service_healthy
    command: /bin/bash
    stdin_open: true
    tty: true

networks:
  spark-net:
    name: spark-net
    driver: bridge

volumes:
  master-tmp:
  worker-tmp:
  client-tmp:
  master-work:
  worker-work:
  client-work:
  mount-client-tmp:
  mount-client-work: 