<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- PostgreSQL Metastore Configuration -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:postgresql://${HIVE_METASTORE_DB_HOST}:${HIVE_METASTORE_DB_PORT}/${HIVE_METASTORE_DB_NAME}</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>org.postgresql.Driver</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>${HIVE_METASTORE_DB_USER}</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>${HIVE_METASTORE_DB_PASSWORD}</value>
    </property>

    <!-- DataNucleus Configuration -->
    <property>
        <name>datanucleus.schema.autoCreateTables</name>
        <value>true</value>
        <description>Enable automatic table creation for DataNucleus</description>
    </property>
    <property>
        <name>datanucleus.schema.autoCreateColumns</name>
        <value>true</value>
        <description>Enable automatic column creation for DataNucleus</description>
    </property>
    <property>
        <name>datanucleus.schema.validateTables</name>
        <value>false</value>
        <description>Disable table validation for DataNucleus</description>
    </property>
    <property>
        <name>datanucleus.schema.autoCreateConstraints</name>
        <value>true</value>
        <description>Enable automatic constraint creation for DataNucleus</description>
    </property>
    <property>
        <name>datanucleus.autoStartMechanism</name>
        <value>SchemaTable</value>
        <description>Use SchemaTable for auto-start mechanism</description>
    </property>
    <property>
        <name>datanucleus.rdbms.datastoreAdapterClassName</name>
        <value>org.datanucleus.store.rdbms.adapter.PostgreSQLAdapter</value>
        <description>PostgreSQL adapter for DataNucleus</description>
    </property>
    <property>
        <name>datanucleus.connectionPoolingType</name>
        <value>None</value>
        <description>Disable connection pooling</description>
    </property>
    <property>
        <name>datanucleus.fixedDatastore</name>
        <value>false</value>
        <description>Allow schema updates</description>
    </property>

    <!-- Hive Server2 Configuration -->
    <property>
        <name>hive.server2.thrift.port</name>
        <value>${HIVE_SERVER2_PORT}</value>
    </property>
    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>${HIVE_SERVER2_BIND_HOST}</value>
    </property>
    <property>
        <name>hive.server2.enable.doAs</name>
        <value>false</value>
    </property>
    <property>
        <name>hive.server2.authentication</name>
        <value>NONE</value>
    </property>
    <property>
        <name>hive.server2.thrift.sasl.qop</name>
        <value>none</value>
    </property>
    <property>
        <name>hive.server2.thrift.sasl.enabled</name>
        <value>false</value>
    </property>

    <!-- Hive Metastore Configuration -->
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://${HIVE_METASTORE_HOST}:${HIVE_METASTORE_PORT}</value>
    </property>
    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
        <description>Disable schema verification</description>
    </property>
    <property>
        <name>metastore.metastore.event.db.notification.api.auth</name>
        <value>false</value>
    </property>

    <!-- Hive Warehouse Configuration -->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>${HIVE_WAREHOUSE_DIR}</value>
        <description>
            Location of default database for the warehouse.
            This directory must exist and be writable by the Hive user.
            Default HDFS path is /user/hive/warehouse, but we use /opt/hive/warehouse
            to avoid HDFS dependency and permission issues.
        </description>
    </property>
    <property>
        <name>hive.exec.scratchdir</name>
        <value>${HIVE_SCRATCH_DIR}</value>
        <description>
            Scratch space for Hive jobs.
            This directory must exist and be writable by the Hive user.
            Default HDFS path is /tmp/hive, but we use /opt/hive/scratch
            to avoid HDFS dependency and permission issues.
        </description>
    </property>
</configuration> 