# Base image
FROM azul/zulu-openjdk:{{ versions.java.full_version }}

# Enable BuildKit features
# syntax=docker/dockerfile:1.4

# Set environment variables
ENV SPARK_VERSION={{ versions.spark }}
ENV HADOOP_VERSION={{ versions.hadoop }}
ENV SCALA_VERSION={{ versions.scala.version }}
ENV SPARK_HOME={{ components.spark.home }}
ENV HADOOP_HOME={{ components.spark.home }}
ENV PATH=$PATH:{{ components.spark.home }}/bin
ENV SPARK_NODE_TYPE=master

# Install system dependencies with cache mounts
RUN --mount=type=cache,target=/var/cache/apt \
    --mount=type=cache,target=/var/lib/apt \
    apt-get update && \
    apt-get install -y --no-install-recommends \
        curl \
        netcat-openbsd \
        procps \
        tini \
        coreutils \
        bash \
        net-tools \
        iproute2 \
        dnsutils \
        htop \
        vim-tiny \
        less \
        file \
        make \
        cmake \
        gcc \
        g++ \
        libssl-dev \
        zlib1g-dev \
        wget \
        python3-pip \
        && rm -rf /var/lib/apt/lists/* && \
    pip3 install --no-cache-dir requests jinja2

# Create scripts directory and copy all Python scripts
COPY --chmod=755 scripts/spark/*.py {{ components.spark.home }}/scripts/

# Download and install components
RUN --mount=type=cache,target=/tmp/downloads \
    --mount=type=cache,target=/tmp/extract \
    mkdir -p /tmp/downloads /tmp/extract && \
    python3 {{ components.spark.home }}/scripts/download-jars.py && \
    # Extract Spark (with Hadoop included)
    tar xzf /tmp/downloads/spark.tgz -C /tmp/extract && \
    mv /tmp/extract/spark-{{ versions.spark }}-bin-hadoop3 {{ components.spark.home }} && \
    # Copy JARs to appropriate directories
    mkdir -p {{ components.spark.home }}/jars/delta {{ components.spark.home }}/jars/aws && \
    mv /tmp/downloads/delta-spark.jar {{ components.spark.home }}/jars/delta/ && \
    mv /tmp/downloads/hadoop-aws.jar {{ components.spark.home }}/jars/aws/ && \
    mv /tmp/downloads/aws-java-sdk-bundle.jar {{ components.spark.home }}/jars/aws/ && \
    rm -rf /tmp/downloads/* /tmp/extract/*

# Configure Spark
COPY --chown=root:root conf/* {{ components.spark.home }}/conf/
RUN python3 {{ components.spark.home }}/scripts/create_spark_defaults.py

# Set Spark configuration
ENV SPARK_CONF_DIR={{ components.spark.home }}/conf
ENV SPARK_CLASSPATH={{ components.spark.home }}/jars/delta/*:{{ components.spark.home }}/jars/aws/*

# Configure S3/MinIO settings
{% if env.AWS_ENDPOINT_URL %}
ENV AWS_ENDPOINT_URL={{ env.AWS_ENDPOINT_URL }}
{% else %}
ENV AWS_ENDPOINT_URL={{ components.minio.endpoint }}
{% endif %}

# AWS credentials must be provided via environment variables at runtime
ENV AWS_ACCESS_KEY_ID=
ENV AWS_SECRET_ACCESS_KEY=

# Configure Delta Lake
ENV SPARK_DELTA_LOG_STORE_CLASS={{ components.spark.delta_log_store }}
{% if env.SPARK_SQL_WAREHOUSE_DIR %}
ENV SPARK_SQL_WAREHOUSE_DIR={{ env.SPARK_SQL_WAREHOUSE_DIR }}
{% else %}
ENV SPARK_SQL_WAREHOUSE_DIR=s3a://{{ components.minio.bucket }}/warehouse
{% endif %}

# Configure S3A settings
ENV SPARK_HADOOP_FS_S3A_PATH_STYLE_ACCESS={{ components.spark.s3a.path_style_access | string | lower }}
ENV SPARK_HADOOP_FS_S3A_CONNECTION_SSL_ENABLED={{ components.spark.s3a.connection_ssl_enabled | string | lower }}

# Configure Hive integration
{% if env.SPARK_SQL_CATALOG_IMPLEMENTATION %}
ENV SPARK_SQL_CATALOG_IMPLEMENTATION={{ env.SPARK_SQL_CATALOG_IMPLEMENTATION }}
{% endif %}

{% if env.HIVE_METASTORE_URI %}
ENV SPARK_HIVE_METASTORE_URI={{ env.HIVE_METASTORE_URI }}
{% endif %}

# Set up spark user and directories
RUN python3 {{ components.spark.home }}/scripts/setup-spark-user.py

# Set working directory
WORKDIR {{ components.spark.home }}

# Expose ports
EXPOSE 4040 7077 8080 8081 10000 10002

# Use tini as init system
ENTRYPOINT ["/usr/bin/tini", "--", "python3", "/opt/spark/scripts/entrypoint.py"]

# Default command (will be overridden by entrypoint.sh based on SPARK_NODE_TYPE)
CMD [] 