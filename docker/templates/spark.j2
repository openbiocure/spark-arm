# Base image
FROM azul/zulu-openjdk:{{ versions.java.full_version }}

# Set environment variables
ENV SPARK_VERSION={{ versions.spark }}
ENV HADOOP_VERSION={{ versions.hadoop }}
ENV SCALA_VERSION={{ versions.scala.version }}
ENV SPARK_HOME=/opt/spark
ENV HADOOP_HOME=/opt/hadoop
ENV PATH=$PATH:$SPARK_HOME/bin:$HADOOP_HOME/bin
ENV SPARK_NODE_TYPE=master

# Install system dependencies with cache mounts
RUN --mount=type=cache,target=/var/cache/apt \
    --mount=type=cache,target=/var/lib/apt \
    apt-get update && \
    apt-get install -y --no-install-recommends \
        curl \
        netcat-openbsd \
        procps \
        tini \
        coreutils \
        bash \
        net-tools \
        iproute2 \
        dnsutils \
        htop \
        vim-tiny \
        less \
        file \
        make \
        cmake \
        gcc \
        g++ \
        libssl-dev \
        zlib1g-dev \
        wget \
        && rm -rf /var/lib/apt/lists/*

# Create scripts directory and copy Python scripts
COPY --chmod=755 scripts/entrypoint.py /opt/spark/scripts/
COPY --chmod=755 scripts/start_master.py /opt/spark/scripts/
COPY --chmod=755 scripts/start_worker.py /opt/spark/scripts/
COPY --chmod=755 scripts/setup-spark-user.sh /opt/spark/scripts/

# Download and install Spark
RUN wget -q {{ urls.spark }} -O /tmp/spark.tgz \
    && tar xzf /tmp/spark.tgz -C /opt \
    && mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} ${SPARK_HOME} \
    && rm /tmp/spark.tgz

# Download and install Hadoop
RUN wget -q {{ urls.hadoop }} -O /tmp/hadoop.tgz \
    && tar xzf /tmp/hadoop.tgz -C /opt \
    && mv /opt/hadoop-${HADOOP_VERSION} ${HADOOP_HOME} \
    && rm /tmp/hadoop.tgz

# Download and install Delta Lake
RUN mkdir -p ${SPARK_HOME}/jars/delta \
    && wget -q {{ urls.delta.core }} -O ${SPARK_HOME}/jars/delta/delta-core.jar \
    && wget -q {{ urls.delta.spark }} -O ${SPARK_HOME}/jars/delta/delta-spark.jar \
    && wget -q {{ urls.delta.storage }} -O ${SPARK_HOME}/jars/delta/delta-storage.jar

# Download AWS and Hadoop dependencies
RUN mkdir -p ${SPARK_HOME}/jars/aws \
    && wget -q {{ urls.aws.bundle }} -O ${SPARK_HOME}/jars/aws/aws-java-sdk-bundle.jar \
    && wget -q {{ urls.aws.s3 }} -O ${SPARK_HOME}/jars/aws/aws-java-sdk-s3.jar \
    && wget -q {{ urls.aws.hadoop }} -O ${SPARK_HOME}/jars/aws/hadoop-aws.jar

# Configure Spark
COPY --chown=root:root spark-defaults.conf ${SPARK_HOME}/conf/
COPY --chown=root:root log4j2.properties ${SPARK_HOME}/conf/

# Set Spark configuration
ENV SPARK_CONF_DIR=${SPARK_HOME}/conf
ENV SPARK_CLASSPATH=${SPARK_HOME}/jars/delta/*:${SPARK_HOME}/jars/aws/*

# Configure S3/MinIO settings
ENV AWS_ENDPOINT_URL={{ env.AWS_ENDPOINT_URL | default(components.minio.endpoint) }}
# These must be provided via environment variables
ENV AWS_ACCESS_KEY_ID={{ env.AWS_ACCESS_KEY_ID }}
ENV AWS_SECRET_ACCESS_KEY={{ env.AWS_SECRET_ACCESS_KEY }}

# Configure Delta Lake
ENV SPARK_DELTA_LOG_STORE_CLASS={{ components.spark.delta_log_store }}
ENV SPARK_SQL_WAREHOUSE_DIR={{ env.SPARK_SQL_WAREHOUSE_DIR | default('s3a://' + components.minio.bucket + '/warehouse') }}

# Configure S3A settings
ENV SPARK_HADOOP_FS_S3A_PATH_STYLE_ACCESS={{ components.spark.s3a.path_style_access | string | lower }}
ENV SPARK_HADOOP_FS_S3A_CONNECTION_SSL_ENABLED={{ components.spark.s3a.connection_ssl_enabled | string | lower }}

# Configure Hive integration
{% if env.SPARK_SQL_CATALOG_IMPLEMENTATION %}
ENV SPARK_SQL_CATALOG_IMPLEMENTATION={{ env.SPARK_SQL_CATALOG_IMPLEMENTATION }}
{% endif %}

{% if env.HIVE_METASTORE_URI %}
ENV SPARK_HIVE_METASTORE_URI={{ env.HIVE_METASTORE_URI }}
{% endif %}

# Set up spark user and directories
RUN /opt/spark/scripts/setup-spark-user.sh

# Set working directory
WORKDIR ${SPARK_HOME}

# Expose ports
EXPOSE 4040 7077 8080 8081 10000 10002

# Use tini as init system
ENTRYPOINT ["/usr/bin/tini", "--", "python3", "/opt/spark/scripts/entrypoint.py"]

# Default command (will be overridden by entrypoint.sh based on SPARK_NODE_TYPE)
CMD [] 