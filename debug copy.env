#Local Mirror
LOCAL_MIRROR_URL=http://172.16.13.237/mirror

# Spark Debug Settings
SPARK_LOG_LEVEL=DEBUG
SPARK_DAEMON_JAVA_OPTS="-Dlog4j.configuration=file:/opt/spark/conf/log4j2.xml -Dspark.log.level=DEBUG"
SPARK_DRIVER_OPTS="-Dlog4j.configuration=file:/opt/spark/conf/log4j2.xml -Dspark.log.level=DEBUG"
SPARK_EXECUTOR_OPTS="-Dlog4j.configuration=file:/opt/spark/conf/log4j2.xml -Dspark.log.level=DEBUG"

# Hive Debug Settings
HIVE_LOG_LEVEL=DEBUG
HIVE_DEBUG=true
HIVE_OPTS="-Dlog4j.configuration=file:/opt/hive/conf/hive-log4j2.properties -Dhive.log.level=DEBUG"
HIVE_SERVER2_OPTS="-Dlog4j.configuration=file:/opt/hive/conf/hive-log4j2.properties -Dhive.log.level=DEBUG"
HIVE_METASTORE_OPTS="-Dlog4j.configuration=file:/opt/hive/conf/hive-log4j2.properties -Dhive.log.level=DEBUG"

# Java Debug Settings
JAVA_TOOL_OPTIONS="-Djava.net.preferIPv4Stack=true -Dfile.encoding=UTF-8"
JAVA_OPTS="-Xms512m -Xmx1024m -XX:+UseG1GC -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/opt/spark/logs"

# Container Debug Settings
SPARK_NODE_TYPE=master  # or worker, or hive
SPARK_MASTER_HOST=spark-arm-master
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=8080
SPARK_WORKER_WEBUI_PORT=8081
SPARK_WORKER_CORES=1
SPARK_WORKER_MEMORY=1G

# Hive Container Settings
HIVE_METASTORE_HOST=hive-metastore
HIVE_METASTORE_PORT=9083
HIVE_SERVER2_PORT=10000
HIVE_SERVER2_THRIFT_BIND_HOST=0.0.0.0
HIVE_METASTORE_URI=thrift://localhost:9083

# PostgreSQL Settings (for Hive Metastore)
POSTGRES_HOST=172.16.14.112
POSTGRES_PORT=5432
POSTGRES_DB=hive
POSTGRES_USER=hive
POSTGRES_PASSWORD=hive

# Additional Debug Settings
SPARK_LOCAL_IP=0.0.0.0
SPARK_PUBLIC_DNS=localhost
SPARK_RPC_AUTHENTICATION_ENABLED=false
SPARK_RPC_ENCRYPTION_ENABLED=false
SPARK_SSL_ENABLED=false
SPARK_AUTHENTICATE=false
SPARK_AUTHENTICATE_SECRET=false
SPARK_AUTHENTICATE_ENABLE_SASL_ENCRYPTION=false

# MinIO Configuration
AWS_ACCESS_KEY_ID=iglIu8yZXZRFipZQDEFI
AWS_SECRET_ACCESS_KEY=J0lqSKgQKKJBfnNnwMHBhinFy1iMxmKGIKh4h6oP
AWS_ENDPOINT_URL=http://172.16.14.201:9000
MINIO_BUCKET=test

# Spark Master URL
SPARK_MASTER_URL="http://localhost:8080/"

# Spark Hive JARs
SPARK_HIVE_JARS="$HOME/spark-hive-jars/hive-common-3.0.0.jar,\
$HOME/spark-hive-jars/hive-cli-3.0.0.jar,\
$HOME/spark-hive-jars/hive-metastore-3.0.0.jar,\
$HOME/spark-hive-jars/hive-exec-3.0.0-core.jar,\
$HOME/spark-hive-jars/hive-serde-3.0.0.jar,\
$HOME/spark-hive-jars/hive-jdbc-3.0.0.jar,\
$HOME/spark-extra-jars/hadoop-aws-3.3.6.jar,\
$HOME/spark-extra-jars/aws-java-sdk-bundle-1.12.262.jar"

# Spark Hive Configuration
SPARK_SQL_CATALOG_IMPLEMENTATION=hive
SPARK_SQL_WAREHOUSE_DIR=s3a://test/warehouse

# Spark S3 Configuration
SPARK_HADOOP_FS_S3A_ENDPOINT=http://172.16.14.201:9000
SPARK_HADOOP_FS_S3A_ACCESS_KEY=iglIu8yZXZRFipZQDEFI
SPARK_HADOOP_FS_S3A_SECRET_KEY=J0lqSKgQKKJBfnNnwMHBhinFy1iMxmKGIKh4h6oP
SPARK_HADOOP_FS_S3A_PATH_STYLE_ACCESS=true
SPARK_HADOOP_FS_S3A_IMPL=org.apache.hadoop.fs.s3a.S3AFileSystem

# Spark JDBC Configuration
SPARK_JDBC_URL=jdbc:postgresql://172.16.14.112:5432/hive
SPARK_JDBC_DRIVER=org.postgresql.Driver
SPARK_JDBC_USER=hive
SPARK_JDBC_PASSWORD=hive 