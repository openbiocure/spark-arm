spark@spark-test-worker:/opt/spark$ env | grep -i "log\|spark"
HIVE_LOG_LEVEL=DEBUG
SPARK_SSL_ENABLED=false
HOSTNAME=spark-test-worker
SPARK_URL_TEMPLATE=http://172.16.13.237/mirror/spark-3.5.5-bin-hadoop3-scala2.13.tgz
SPARK_MASTER_HOST=spark-arm-master
SPARK_AUTHENTICATE=false
SPARK_MASTER_WEBUI_PORT=8080
JAVA_OPTS=-Xms512m -Xmx1024m -XX:+UseG1GC -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/opt/spark/logs
SPARK_RPC_ENCRYPTION_ENABLED=false
HIVE_OPTS=-Dlog4j.configuration=file:/opt/hive/conf/hive-log4j2.properties -Dhive.log.level=DEBUG
SPARK_DAEMON_JAVA_OPTS=-Dlog4j.configuration=file:/opt/spark/conf/log4j2.xml -Dspark.log.level=DEBUG
SPARK_NODE_TYPE=worker
SPARK_HIVE_JARS=/home/pi/spark-hive-jars/hive-common-2.3.9.jar,\
/home/pi/spark-hive-jars/hive-cli-2.3.9.jar,\
/home/pi/spark-hive-jars/hive-metastore-2.3.9.jar,\
/home/pi/spark-hive-jars/hive-exec-2.3.9-core.jar,\
/home/pi/spark-hive-jars/hive-serde-2.3.9.jar,\
/home/pi/spark-hive-jars/hive-jdbc-2.3.9.jar,\
/home/pi/spark-extra-jars/hadoop-aws-3.3.4.jar,\
/home/pi/spark-extra-jars/aws-java-sdk-bundle-1.12.262.jar
PWD=/opt/spark
SPARK_SQL_WAREHOUSE_DIR=s3a://test/warehouse
HIVE_SERVER2_OPTS=-Dlog4j.configuration=file:/opt/hive/conf/hive-log4j2.properties -Dhive.log.level=DEBUG
SPARK_WORKER_WEBUI_PORT=8081
SPARK_RPC_AUTHENTICATION_ENABLED=false
HOME=/home/spark
SPARK_WORKER_CORES=1
SPARK_DRIVER_OPTS=-Dlog4j.configuration=file:/opt/spark/conf/log4j2.xml -Dspark.log.level=DEBUG
SPARK_AUTHENTICATE_SECRET=false
SPARK_JDBC_USER=hive
SPARK_JDBC_URL=jdbc:postgresql://172.16.14.112:5432/hive
SPARK_MASTER_PORT=7077
SPARK_LOG_LEVEL=DEBUG
SPARK_MASTER_URL=spark://spark-test:7077
DELTA_URL_TEMPLATE=https://repo1.maven.org/maven2/io/delta/delta-spark_2.13/3.3.1/delta-spark_2.13-3.3.1.jar
SPARK_HADOOP_FS_S3A_ACCESS_KEY=iglIu8yZXZRFipZQDEFI
SPARK_PUBLIC_DNS=localhost
SPARK_VERSION=3.5.5
SPARK_DRIVER_DIR=/opt/spark/tmp
SPARK_AUTHENTICATE_ENABLE_SASL_ENCRYPTION=false
SPARK_LOCAL_DIRS=/opt/spark/tmp
SPARK_HADOOP_FS_S3A_SECRET_KEY=J0lqSKgQKKJBfnNnwMHBhinFy1iMxmKGIKh4h6oP
SPARK_HADOOP_FS_S3A_PATH_STYLE_ACCESS=true
SPARK_HOME=/opt/spark
SPARK_LOCAL_IP=0.0.0.0
SPARK_CONF_DIR=/opt/spark/conf
SPARK_DIST_CLASSPATH=/opt/spark/jars/*
SPARK_JDBC_PASSWORD=hive
SPARK_HADOOP_FS_S3A_ENDPOINT=http://172.16.14.201:9000
SPARK_WORKER_DIR=/opt/spark/tmp
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/spark/bin
SPARK_HADOOP_FS_S3A_IMPL=org.apache.hadoop.fs.s3a.S3AFileSystem
SPARK_EXECUTOR_OPTS=-Dlog4j.configuration=file:/opt/spark/conf/log4j2.xml -Dspark.log.level=DEBUG
SPARK_CLASSPATH=/opt/spark/jars/*
HIVE_METASTORE_OPTS=-Dlog4j.configuration=file:/opt/hive/conf/hive-log4j2.properties -Dhive.log.level=DEBUG
SPARK_RESOURCES_DIR=/opt/spark
SPARK_SQL_CATALOG_IMPLEMENTATION=hive
SPARK_JDBC_DRIVER=org.postgresql.Driver
SPARK_WORKER_MEMORY=1G
spark@spark-test-worker:/opt/spark$