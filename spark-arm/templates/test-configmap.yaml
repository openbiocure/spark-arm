# GENERATED FILE - DO NOT EDIT DIRECTLY
# This file is automatically generated by scripts/update-test-configmap.sh
# Any changes should be made to the source files in the tests/ directory
# Last generated: Sun 11 May 2025 19:39:35 +04

apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-test-scripts
  namespace: spark
data:
  test_spark_cluster.py: |
    from pyspark.sql import SparkSession
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType
    import os
    import time
    import glob
    
    def verify_delta_jars():
        """Verify that Delta Lake JARs are present and in classpath."""
        print("\n=== Verifying Delta Lake JARs ===")
        jar_path = "/opt/spark/jars"
        delta_jars = glob.glob(f"{jar_path}/delta-*.jar")
        
        print(f"Found Delta JARs in {jar_path}:")
        for jar in delta_jars:
            print(f"- {os.path.basename(jar)}")
        
        if not delta_jars:
            raise RuntimeError("No Delta Lake JARs found in /opt/spark/jars/")
        
        print("\nClasspath environment variables:")
        print(f"SPARK_CLASSPATH: {os.getenv('SPARK_CLASSPATH', 'Not set')}")
        print(f"SPARK_HOME: {os.getenv('SPARK_HOME', 'Not set')}")
        print(f"SPARK_DAEMON_JAVA_OPTS: {os.getenv('SPARK_DAEMON_JAVA_OPTS', 'Not set')}")
    
    def create_spark_session():
        """Create a Spark session with all necessary configurations."""
        # Verify Delta JARs before creating session
        verify_delta_jars()
        
        # Get list of Delta JARs
        jar_path = "/opt/spark/jars"
        delta_jars = glob.glob(f"{jar_path}/delta-*.jar")
        delta_jars_str = ",".join(delta_jars)
        
        print(f"\nUsing Delta JARs: {delta_jars_str}")
        
        # Enable debug logging
        print("\nSetting up Spark session with debug logging...")
        
        return (SparkSession.builder
                .appName("SparkClusterTest")
                .master(os.getenv("SPARK_MASTER_URL", "spark://spark-arm-master:7077"))
                .config("spark.jars", delta_jars_str)
                .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
                .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
                .config("spark.driver.extraJavaOptions", "-Dlog4j.configuration=file:/opt/spark/conf/log4j2.xml -Dlog4j2.debug=true")
                .config("spark.executor.extraJavaOptions", "-Dlog4j.configuration=file:/opt/spark/conf/log4j2.xml -Dlog4j2.debug=true")
                .config("spark.driver.log.level", "DEBUG")
                .config("spark.executor.log.level", "DEBUG")
                .config("spark.sql.warehouse.dir", "s3a://${MINIO_BUCKET}/warehouse")
                .config("spark.sql.catalogImplementation", "hive")
                .config("spark.hadoop.javax.jdo.option.ConnectionURL", "jdbc:postgresql://${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}")
                .config("spark.hadoop.javax.jdo.option.ConnectionDriverName", "org.postgresql.Driver")
                .config("spark.hadoop.javax.jdo.option.ConnectionUserName", "${POSTGRES_USER}")
                .config("spark.hadoop.javax.jdo.option.ConnectionPassword", "${POSTGRES_PASSWORD}")
                .config("spark.hadoop.datanucleus.schema.autoCreateAll", "true")
                .config("spark.hadoop.datanucleus.autoCreateSchema", "true")
                .config("spark.hadoop.datanucleus.fixedDatastore", "false")
                .config("spark.hadoop.datanucleus.autoCreateTables", "true")
                .config("spark.delta.logStore.class", "org.apache.spark.sql.delta.storage.S3SingleDriverLogStore")
                .config("spark.delta.merge.repartitionBeforeWrite", "true")
                .config("spark.delta.autoOptimize.optimizeWrite", "true")
                .config("spark.delta.autoOptimize.autoCompact", "true")
                .config("spark.delta.storage.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
                .config("spark.delta.storage.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
                .config("spark.delta.warehouse.dir", "s3a://${MINIO_BUCKET}/delta")
                .config("spark.delta.optimizeWrite.enabled", "true")
                .config("spark.delta.autoCompact.enabled", "true")
                .config("spark.delta.optimizeWrite.numShuffleBlocks", "200")
                .config("spark.delta.optimizeWrite.targetFileSize", "128m")
                .config("spark.delta.concurrent.writes.enabled", "true")
                .config("spark.delta.concurrent.writes.maxConcurrentWrites", "10")
                .config("spark.delta.schema.autoMerge.enabled", "true")
                .config("spark.delta.timeTravel.enabled", "true")
                .config("spark.delta.timeTravel.retentionPeriod", "168h")
                .getOrCreate())
    
    def test_basic_spark(spark):
        """Test basic Spark functionality."""
        print("\n=== Testing Basic Spark Functionality ===")
        
        # Create a simple DataFrame
        data = [("Alice", 1), ("Bob", 2), ("Charlie", 3)]
        schema = StructType([
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])
        df = spark.createDataFrame(data, schema)
        
        # Test DataFrame operations
        print("Created DataFrame:")
        df.show()
        
        # Test SQL
        df.createOrReplaceTempView("people")
        result = spark.sql("SELECT * FROM people WHERE age > 1")
        print("\nSQL Query Result:")
        result.show()
        
        return True
    
    def test_minio_connectivity(spark):
        """Test MinIO connectivity and basic S3 operations."""
        print("\n=== Testing MinIO Connectivity ===")
        
        # Create a test DataFrame
        data = [("test1", 1), ("test2", 2)]
        df = spark.createDataFrame(data, ["key", "value"])
        
        # Get bucket name from environment
        bucket = os.getenv("MINIO_BUCKET", "spark-data")
        test_path = f"s3a://{bucket}/test/minio_test"
        
        # Write to MinIO
        print(f"Writing test data to {test_path}")
        df.write.mode("overwrite").parquet(test_path)
        
        # Read from MinIO
        print("Reading test data from MinIO")
        read_df = spark.read.parquet(test_path)
        print("Data read from MinIO:")
        read_df.show()
        
        return True
    
    def test_hive_metastore(spark):
        """Test Hive metastore connectivity and operations."""
        print("\n=== Testing Hive Metastore ===")
        
        # Create a test table
        table_name = "test_hive_table"
        data = [("hive1", 1), ("hive2", 2)]
        df = spark.createDataFrame(data, ["name", "value"])
        
        # Write to Hive
        print(f"Creating Hive table: {table_name}")
        df.write.mode("overwrite").saveAsTable(table_name)
        
        # Read from Hive
        print("Reading from Hive table")
        result = spark.sql(f"SELECT * FROM {table_name}")
        print("Data from Hive table:")
        result.show()
        
        return True
    
    def test_delta_lake(spark):
        """Test Delta Lake functionality."""
        print("\n=== Testing Delta Lake ===")
        
        # Get bucket name from environment
        bucket = os.getenv("MINIO_BUCKET", "spark-data")
        delta_path = f"s3a://{bucket}/delta/test_delta"
        
        # Create initial data
        data = [("delta1", 1), ("delta2", 2)]
        df = spark.createDataFrame(data, ["name", "value"])
        
        # Write as Delta table
        print(f"Creating Delta table at {delta_path}")
        df.write.format("delta").mode("overwrite").save(delta_path)
        
        # Read Delta table
        print("Reading Delta table")
        delta_df = spark.read.format("delta").load(delta_path)
        print("Initial Delta table data:")
        delta_df.show()
        
        # Perform an update
        print("Performing Delta table update")
        new_data = [("delta3", 3), ("delta4", 4)]
        update_df = spark.createDataFrame(new_data, ["name", "value"])
        update_df.write.format("delta").mode("append").save(delta_path)
        
        # Read updated data
        print("Reading updated Delta table")
        updated_df = spark.read.format("delta").load(delta_path)
        print("Updated Delta table data:")
        updated_df.show()
        
        return True
    
    def main():
        """Run all tests."""
        print("Starting Spark Cluster Tests...")
        
        # Create Spark session
        spark = create_spark_session()
        
        try:
            # Run all tests
            tests = [
                ("Basic Spark", test_basic_spark),
                ("MinIO", test_minio_connectivity),
                ("Hive", test_hive_metastore),
                ("Delta Lake", test_delta_lake)
            ]
            
            results = []
            for test_name, test_func in tests:
                print(f"\nRunning {test_name} test...")
                try:
                    result = test_func(spark)
                    results.append((test_name, result, None))
                    print(f"✓ {test_name} test passed")
                except Exception as e:
                    results.append((test_name, False, str(e)))
                    print(f"✗ {test_name} test failed: {str(e)}")
            
            # Print summary
            print("\n=== Test Summary ===")
            all_passed = True
            for test_name, passed, error in results:
                status = "PASSED" if passed else f"FAILED: {error}"
                print(f"{test_name}: {status}")
                if not passed:
                    all_passed = False
            
            return 0 if all_passed else 1
            
        finally:
            spark.stop()
    
    if __name__ == "__main__":
        exit(main()) 
