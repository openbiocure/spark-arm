apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ include "spark-arm.fullname" . }}-worker
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "spark-arm.labels" . | nindent 4 }}
    app.kubernetes.io/component: worker
spec:
  serviceName: {{ include "spark-arm.fullname" . }}-worker
  # Set number of worker replicas from values.yaml
  replicas: {{ .Values.worker.replicaCount | default 1 }}
  selector:
    matchLabels:
      {{- include "spark-arm.selectorLabels" . | nindent 6 }}
      app.kubernetes.io/component: worker
  template:
    metadata:
      labels:
        {{- include "spark-arm.selectorLabels" . | nindent 8 }}
        app.kubernetes.io/component: worker
    spec:
      {{- if .Values.serviceAccount.create }}
      serviceAccountName: {{ include "spark-arm.serviceAccountName" . }}
      {{- end }}
      nodeSelector:
        kubernetes.io/os: linux
      volumes:
        - name: spark-logs
          persistentVolumeClaim:
            claimName: {{ include "spark-arm.fullname" . }}-logs
      containers:
        - name: spark
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          command: ["/opt/spark/bin/spark-class"]
          args: ["org.apache.spark.deploy.worker.Worker", "spark://{{ include "spark-arm.fullname" . }}-master:7077"]
          env:
            - name: SPARK_HOME
              value: /opt/spark
            - name: SPARK_NODE_TYPE
              value: "worker"
            - name: SPARK_LOCAL_HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: SPARK_PUBLIC_DNS
              value: spark-arm-worker
            - name: SPARK_WORKER_OPTS
              value: >-
                -Dspark.worker.cleanup.enabled=true
                -Dspark.hadoop.fs.s3a.access.key=$(MINIO_ACCESS_KEY)
                -Dspark.hadoop.fs.s3a.secret.key=$(MINIO_SECRET_KEY)
                -Dspark.hadoop.datanucleus.autoCreateSchema="true"
                -Dspark.hadoop.datanucleus.autoCreateTables="true"
                -Dspark.hadoop.datanucleus.fixedDatastore="false"
                -Dspark.hadoop.datanucleus.schema.autoCreateAll="true"
                -Dspark.hadoop.javax.jdo.option.ConnectionDriverName="org.postgresql.Driver"
                -Dspark.hadoop.javax.jdo.option.ConnectionPassword="${POSTGRES_PASSWORD}"
                -Dspark.hadoop.javax.jdo.option.ConnectionURL="jdbc:postgresql://${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}"
                -Dspark.hadoop.javax.jdo.option.ConnectionUserName="${POSTGRES_USER}"
                -Dspark.sql.catalogImplementation="hive"
                -Dspark.sql.warehouse.dir="s3a://${MINIO_BUCKET}/warehouse"
                -Dspark.delta.autoCompact.enabled="true"
                -Dspark.delta.autoOptimize.autoCompact="true"
                -Dspark.delta.autoOptimize.optimizeWrite="true"
                -Dspark.delta.concurrent.writes.enabled="true"
                -Dspark.delta.concurrent.writes.maxConcurrentWrites="10"
                -Dspark.delta.logStore.class="org.apache.spark.sql.delta.storage.S3SingleDriverLogStore"
                -Dspark.delta.merge.repartitionBeforeWrite="true"
                -Dspark.delta.optimizeWrite.enabled="true"
                -Dspark.delta.optimizeWrite.numShuffleBlocks="200"
                -Dspark.delta.optimizeWrite.targetFileSize="128m"
                -Dspark.delta.schema.autoMerge.enabled="true"
                -Dspark.delta.storage.s3.impl="org.apache.hadoop.fs.s3a.S3AFileSystem"
                -Dspark.delta.storage.s3a.impl="org.apache.hadoop.fs.s3a.S3AFileSystem"
                -Dspark.delta.timeTravel.enabled="true"
                -Dspark.delta.timeTravel.retentionPeriod="168h"
                -Dspark.delta.warehouse.dir="s3a://${MINIO_BUCKET}/delta"
                -Dspark.sql.catalog.spark_catalog="org.apache.spark.sql.delta.catalog.DeltaCatalog"
                -Dspark.sql.extensions="io.delta.sql.DeltaSparkSessionExtension"
            - name: MINIO_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: spark-arm-minio
                  key: access-key
            - name: MINIO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: spark-arm-minio
                  key: secret-key
            - name: POSTGRES_HOST
              value: "postgresql"
            - name: POSTGRES_PORT
              value: "5432"
            - name: POSTGRES_DB
              valueFrom:
                secretKeyRef:
                  name: spark-arm-postgres
                  key: database
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: spark-arm-postgres
                  key: username
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: spark-arm-postgres
                  key: password
            - name: HIVE_METASTORE_HOST
              value: "postgresql"
            - name: SPARK_WORKER_CORES
              value: "4"
            - name: SPARK_WORKER_MEMORY
              value: "2048m"
            - name: SPARK_WORKER_WEBUI_PORT
              value: "8081"
            - name: SPARK_DAEMON_JAVA_OPTS
              value: "-Dspark.worker.cleanup.enabled=true"
          ports:
            - containerPort: {{ .Values.service.port }}
              name: spark
            - containerPort: 8081
              name: webui
          volumeMounts:
            - name: spark-logs
              mountPath: /opt/spark/logs
          resources:
            {{- toYaml .Values.worker.resources | nindent 12 }}
          startupProbe:
            tcpSocket:
              port: webui
            initialDelaySeconds: 10
            periodSeconds: 3
            failureThreshold: 30
            timeoutSeconds: 2
          livenessProbe:
            tcpSocket:
              port: webui
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
          readinessProbe:
            tcpSocket:
              port: webui
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3 