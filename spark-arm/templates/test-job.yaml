{{- if .Values.test.enabled }}
apiVersion: batch/v1
kind: Job
metadata:
  name: spark-test-job
  namespace: spark
spec:
  template:
    spec:
      initContainers:
      - name: copy-jars
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        command: ["/bin/bash", "-c"]
        args:
        - |
          echo "Copying JARs from container image..."
          cp -r /opt/spark/jars/* /jars/
          echo "JARs copied successfully"
        volumeMounts:
        - name: spark-jars
          mountPath: /jars
      containers:
      - name: spark-test
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        command: 
        - /bin/bash
        - -c
        - |
          # Set up classpath with explicit paths to Delta JARs
          DELTA_JARS="/opt/spark/jars/delta-contribs_2.13-3.3.1.jar,/opt/spark/jars/delta-hive_2.13-3.3.1.jar,/opt/spark/jars/delta-standalone_2.13-3.3.1.jar,/opt/spark/jars/delta-storage-3.3.1.jar"
          
          # Set classpath environment variables
          export SPARK_CLASSPATH="${DELTA_JARS}"
          export SPARK_DRIVER_CLASSPATH="${DELTA_JARS}"
          export SPARK_EXECUTOR_CLASSPATH="${DELTA_JARS}"
          
          # Set Java options
          export SPARK_DRIVER_OPTS="-Dspark.driver.extraClassPath=${DELTA_JARS}"
          export SPARK_EXECUTOR_OPTS="-Dspark.executor.extraClassPath=${DELTA_JARS}"
          
          # List JARs and classpath for debugging
          echo "=== Available JARs ==="
          ls -l /opt/spark/jars/delta-*
          echo -e "\n=== Classpath ==="
          echo "SPARK_CLASSPATH: ${SPARK_CLASSPATH}"
          echo "SPARK_DRIVER_CLASSPATH: ${SPARK_DRIVER_CLASSPATH}"
          echo "SPARK_EXECUTOR_CLASSPATH: ${SPARK_EXECUTOR_CLASSPATH}"
          echo -e "\n=== Java Options ==="
          echo "SPARK_DRIVER_OPTS: ${SPARK_DRIVER_OPTS}"
          echo "SPARK_EXECUTOR_OPTS: ${SPARK_EXECUTOR_OPTS}"
          
          # Build and run Scala tests
          cd /opt/spark/tests
          sbt clean assembly
          spark-submit \
            --class org.openbiocure.spark.TestSparkCluster \
            target/scala-2.13/spark-arm-tests-assembly-1.0.0.jar
        env:
        - name: SPARK_HOME
          value: /opt/spark
        - name: SPARK_MASTER_URL
          value: spark://spark-arm-master:7077
        - name: SPARK_LOG_LEVEL
          value: "DEBUG"
        - name: SPARK_DRIVER_LOG_LEVEL
          value: "DEBUG"
        - name: SPARK_EXECUTOR_LOG_LEVEL
          value: "DEBUG"
        - name: MINIO_ENDPOINT
          value: "{{ .Values.minio.endpoint }}"
        - name: MINIO_ACCESS_KEY
          value: "{{ .Values.minio.credentials.accessKey }}"
        - name: MINIO_SECRET_KEY
          value: "{{ .Values.minio.credentials.secretKey }}"
        - name: MINIO_BUCKET
          value: "{{ .Values.minio.bucket }}"
        - name: POSTGRES_HOST
          value: "{{ .Values.hive.metastore.host }}"
        - name: POSTGRES_PORT
          value: {{ .Values.hive.metastore.port | quote }}
        - name: POSTGRES_DB
          value: "{{ .Values.hive.metastore.database }}"
        - name: POSTGRES_USER
          value: "{{ .Values.hive.metastore.username }}"
        - name: POSTGRES_PASSWORD
          value: "{{ .Values.hive.metastore.password }}"
        - name: SPARK_DAEMON_JAVA_OPTS
          value: >-
            -Dlog4j.configuration=file:/opt/spark/conf/log4j2.xml
            -Dlog4j2.debug=true
            -Dspark.driver.extraClassPath=/opt/spark/jars/delta-*.jar:/opt/spark/jars/*
            -Dspark.executor.extraClassPath=/opt/spark/jars/delta-*.jar:/opt/spark/jars/*
            {{- if .Values.delta.enabled }}
            {{- range $key, $value := .Values.delta.spark.hadoop }}
            -D{{ $key }}={{ $value | quote }}
            {{- end }}
            {{- end }}
        volumeMounts:
        - name: test-scripts
          mountPath: /opt/spark/tests
        - name: spark-jars
          mountPath: /opt/spark/jars
      volumes:
      - name: test-scripts
        configMap:
          name: spark-test-scripts
      - name: spark-jars
        emptyDir: {}
      restartPolicy: {{ .Values.test.job.restartPolicy }}
  backoffLimit: {{ .Values.test.job.backoffLimit }}
{{- end }} 