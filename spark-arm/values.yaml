# Release name and namespace
nameOverride: ""
fullnameOverride: ""
namespace: spark

# Test job configuration
test:
  enabled: false # Set to true only when running tests
  job:
    backoffLimit: 1
    restartPolicy: Never

# MinIO configuration
minio:
  enabled: true
  # External MinIO service configuration
  endpoint: "${AWS_ENDPOINT_URL}"
  bucket: "${MINIO_BUCKET:-spark-data}" # Default to spark-data if not set
  # MinIO credentials (should be provided via secret in production)
  credentials:
    accessKey: "${AWS_ACCESS_KEY_ID}" # Leave empty, should be provided via secret
    secretKey: "${AWS_SECRET_ACCESS_KEY}" # Leave empty, should be provided via secret
  # Spark integration settings
  spark:
    hadoop:
      fs.s3a.endpoint: "${AWS_ENDPOINT_URL}"
      fs.s3a.access.key: "${AWS_ACCESS_KEY_ID}"
      fs.s3a.secret.key: "${AWS_SECRET_ACCESS_KEY}"
      fs.s3a.path.style.access: "true"
      fs.s3a.connection.ssl.enabled: "false"
      fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
      # Additional settings for external access
      fs.s3a.connection.maximum: "100"
      fs.s3a.connection.timeout: "10000"
      fs.s3a.connection.establish.timeout: "5000"
      fs.s3a.aws.credentials.provider: "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
      # Delta Lake specific S3 settings
      fs.s3a.committer.name: "directory"
      fs.s3a.committer.staging.conflict-mode: "append"
      fs.s3a.committer.staging.tmp.path: "/tmp/staging"
      fs.s3a.committer.threads: "4"
      fs.s3a.committer.directory.cleanup.enabled: "true"

# Master configuration
master:
  enabled: true
  resources:
    limits:
      cpu: "1"
      memory: "1Gi"
    requests:
      cpu: "500m"
      memory: "512Mi"
  env:
    SPARK_MASTER_PORT: "7077"
    SPARK_MASTER_WEBUI_PORT: "8080"

# Worker configuration
worker:
  enabled: true
  # Number of worker pods to create
  replicaCount: 3 # Must be set to 3 for three worker nodes
  cores: "4"
  memory: "2048m"
  resources:
    limits:
      cpu: "4"
      memory: "2Gi"
    requests:
      cpu: "2"
      memory: "1Gi"
  env:
    SPARK_MASTER_PORT: "7077"
    SPARK_WORKER_CORES: "4"
    SPARK_WORKER_MEMORY: "2048m"

# Image configuration
image:
  repository: ghcr.io/openbiocure/spark-arm
  # Default tag, will be overridden by version from tag file during deployment
  tag: stable
  pullPolicy: IfNotPresent

# Service configuration
service:
  type: ClusterIP
  port: 7077
  uiPort: 8080

# Storage configuration
storage:
  className: local-path
  size: 10Gi
  accessMode: ReadWriteOnce

# ServiceAccount configuration
serviceAccount:
  create: true
  name: spark
  automount: true
  annotations: {}

# Ingress configuration
ingress:
  enabled: false
  className: "traefik"
  annotations:
    traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
    traefik.ingress.kubernetes.io/router.tls: "true"
    kubernetes.io/ingress.class: traefik
  hosts:
    - host: "spark.cluster.local"
      paths:
        - path: /
          pathType: Prefix

# Traefik specific configuration
traefik:
  enabled: true
  dashboard:
    enabled: true
    domain: "spark-dashboard.cluster.local"
  ssl:
    enabled: true
    existingSecret: ""
    createSecret: false
    cert: ""
    key: ""
    certManager:
      enabled: true
      issuerName: "letsencrypt-prod"
      issuerKind: "ClusterIssuer"
  middlewares:
    auth:
      enabled: false
      type: "basicAuth"
      users: []
    cors:
      enabled: false
      allowedOrigins: ["*"]
      allowedMethods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
    rateLimit:
      enabled: false
      average: 100
      burst: 50

# Autoscaling configuration
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

# Hive Metastore Configuration
hive:
  enabled: true
  metastore:
    type: postgresql
    host: "${POSTGRES_HOST}"
    port: 5432
    database: "hive"
    username: "${POSTGRES_USER}"
    password: "${POSTGRES_PASSWORD}"
  spark:
    hadoop:
      # Spark SQL Hive settings
      spark.sql.warehouse.dir: "s3a://${MINIO_BUCKET}/warehouse"
      spark.sql.catalogImplementation: "hive"
      # Direct PostgreSQL connection for Hive metastore
      spark.hadoop.javax.jdo.option.ConnectionURL: "jdbc:postgresql://${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}"
      spark.hadoop.javax.jdo.option.ConnectionDriverName: "org.postgresql.Driver"
      spark.hadoop.javax.jdo.option.ConnectionUserName: "${POSTGRES_USER}"
      spark.hadoop.javax.jdo.option.ConnectionPassword: "${POSTGRES_PASSWORD}"
      # Disable schema verification and SASL for simplicity
      spark.hadoop.datanucleus.schema.autoCreateAll: "true"
      spark.hadoop.datanucleus.autoCreateSchema: "true"
      spark.hadoop.datanucleus.fixedDatastore: "false"
      spark.hadoop.datanucleus.autoCreateTables: "true"

# Delta Lake Configuration
delta:
  enabled: true
  spark:
    hadoop:
      # Delta Lake specific settings
      spark.sql.extensions: "io.delta.sql.DeltaSparkSessionExtension"
      spark.sql.catalog.spark_catalog: "org.apache.spark.sql.delta.catalog.DeltaCatalog"
      # Delta Lake table properties
      spark.delta.logStore.class: "org.apache.spark.sql.delta.storage.S3SingleDriverLogStore"
      spark.delta.merge.repartitionBeforeWrite: "true"
      spark.delta.autoOptimize.optimizeWrite: "true"
      spark.delta.autoOptimize.autoCompact: "true"
      # Delta Lake S3 settings
      spark.delta.storage.s3.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
      spark.delta.storage.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
      # Delta Lake table paths
      spark.delta.warehouse.dir: "s3a://${MINIO_BUCKET}/delta"
      # Delta Lake performance settings
      spark.delta.optimizeWrite.enabled: "true"
      spark.delta.autoCompact.enabled: "true"
      spark.delta.optimizeWrite.numShuffleBlocks: "200"
      spark.delta.optimizeWrite.targetFileSize: "128m"
      # Delta Lake concurrent writes settings
      spark.delta.concurrent.writes.enabled: "true"
      spark.delta.concurrent.writes.maxConcurrentWrites: "10"
      # Delta Lake schema evolution
      spark.delta.schema.autoMerge.enabled: "true"
      # Delta Lake time travel settings
      spark.delta.timeTravel.enabled: "true"
      spark.delta.timeTravel.retentionPeriod: "168h" # 7 days
