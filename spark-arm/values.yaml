# Release name and namespace
nameOverride: ""
fullnameOverride: ""
namespace: spark

# Test job configuration
test:
  enabled: false # Set to true only when running tests
  job:
    backoffLimit: 1
    restartPolicy: Never

# MinIO configuration
minio:
  enabled: true
  # External MinIO service configuration
  endpoint: "${AWS_ENDPOINT_URL}"
  bucket: "${MINIO_BUCKET:-spark-data}" # Default to spark-data if not set
  # MinIO credentials (provided via secret)
  credentials:
    accessKey:
      secretKeyRef:
        name: spark-arm-minio
        key: access-key
    secretKey:
      secretKeyRef:
        name: spark-arm-minio
        key: secret-key
  # Spark integration settings
  spark:
    hadoop:
      fs.s3a.endpoint: "${AWS_ENDPOINT_URL}"
      fs.s3a.access.key:
        secretKeyRef:
          name: spark-arm-minio
          key: s3a-access-key
      fs.s3a.secret.key:
        secretKeyRef:
          name: spark-arm-minio
          key: s3a-secret-key
      fs.s3a.path.style.access: "true"
      fs.s3a.connection.ssl.enabled: "false"
      fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
      # Additional settings for external access
      fs.s3a.connection.maximum: "100"
      fs.s3a.connection.timeout: "10000"
      fs.s3a.connection.establish.timeout: "5000"
      fs.s3a.aws.credentials.provider: "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
      # Delta Lake specific S3 settings
      fs.s3a.committer.name: "directory"
      fs.s3a.committer.staging.conflict-mode: "append"
      fs.s3a.committer.staging.tmp.path: "/tmp/staging"
      fs.s3a.committer.threads: "4"
      fs.s3a.committer.directory.cleanup.enabled: "true"

# Master configuration
master:
  enabled: true
  resources:
    limits:
      cpu: "1"
      memory: "1Gi"
    requests:
      cpu: "500m"
      memory: "512Mi"
  env:
    SPARK_MASTER_PORT: "7077"
    SPARK_MASTER_WEBUI_PORT: "8080"

# Worker configuration
worker:
  enabled: true
  # Number of worker pods to create
  replicaCount: 3 # Must be set to 3 for three worker nodes
  cores: "4"
  memory: "2048m"
  resources:
    limits:
      cpu: "4"
      memory: "2Gi"
    requests:
      cpu: "2"
      memory: "1Gi"
  env:
    SPARK_MASTER_PORT: "7077"
    SPARK_WORKER_CORES: "4"
    SPARK_WORKER_MEMORY: "2048m"

# Image configuration
image:
  repository: ghcr.io/openbiocure/spark-arm
  # Default tag, will be overridden by version from tag file during deployment
  tag: ${VERSION}
  pullPolicy: IfNotPresent

# Service configuration
service:
  type: ClusterIP
  port: 7077
  uiPort: 8080

# Storage configuration
storage:
  className: local-path
  size: 10Gi
  accessMode: ReadWriteOnce

# ServiceAccount configuration
serviceAccount:
  create: true
  name: spark
  automount: true
  annotations: {}

# Ingress configuration
ingress:
  enabled: false
  className: "traefik"
  annotations:
    traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
    traefik.ingress.kubernetes.io/router.tls: "true"
    kubernetes.io/ingress.class: traefik
  hosts:
    - host: "spark.cluster.local"
      paths:
        - path: /
          pathType: Prefix

# Traefik specific configuration
traefik:
  enabled: true
  dashboard:
    enabled: true
    domain: "spark-dashboard.cluster.local"
  ssl:
    enabled: true
    existingSecret: ""
    createSecret: false
    cert: ""
    key: ""
    certManager:
      enabled: true
      issuerName: "letsencrypt-prod"
      issuerKind: "ClusterIssuer"
  middlewares:
    auth:
      enabled: false
      type: "basicAuth"
      users: []
    cors:
      enabled: false
      allowedOrigins: ["*"]
      allowedMethods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
    rateLimit:
      enabled: false
      average: 100
      burst: 50

# Autoscaling configuration
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

# Hive Metastore Configuration
hive:
  enabled: true
  replicaCount: 1
  image:
    repository: ghcr.io/openbiocure/hive-arm
    tag: ${VERSION}
    pullPolicy: IfNotPresent
  metastore:
    enabled: true
    host: "${POSTGRES_HOST}"
    port: "${POSTGRES_PORT}"
    user: "${POSTGRES_USER}"
    password: "${POSTGRES_PASSWORD}"
    dbName: "${POSTGRES_DB:-hive}"
  server2:
    enabled: true
    port: 10000
    authentication: NONE
    sasl:
      enabled: false
      qop: auth
  warehouseDir: "s3a://${MINIO_BUCKET}/warehouse"
  scratchDir: "s3a://${MINIO_BUCKET}/scratch"
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1000m
      memory: 2Gi
  extraEnv:
    - name: HADOOP_HOME
      value: "/opt/hadoop"
    - name: HADOOP_CONF_DIR
      value: "/opt/hadoop/etc/hadoop"
    - name: HIVE_HOME
      value: "/opt/hive"
    - name: HIVE_CONF_DIR
      value: "/opt/hive/conf"
    - name: PATH
      value: "/opt/hadoop/bin:/opt/hive/bin:${PATH}"
  extraVolumes:
    - name: hadoop-libs
      mountPath: /opt/hadoop/lib
      subPath: lib
    - name: hadoop-conf
      mountPath: /opt/hadoop/etc/hadoop
      subPath: etc/hadoop
    - name: hive-conf
      mountPath: /opt/hive/conf
      subPath: conf
  extraVolumeMounts:
    - name: hadoop-libs
      mountPath: /opt/hadoop/lib
      subPath: lib
    - name: hadoop-conf
      mountPath: /opt/hadoop/etc/hadoop
      subPath: etc/hadoop
    - name: hive-conf
      mountPath: /opt/hive/conf
      subPath: conf
  nodeSelector: {}
  affinity: {}
  tolerations: []
  service:
    type: ClusterIP
    metastorePort: 9083
    server2Port: 10000
  persistence:
    enabled: true
    storageClass: "local-path"
    size: 10Gi
    accessMode: ReadWriteOnce
  logging:
    level: DEBUG
    log4j2:
      status: INFO
      monitorInterval: 30
  spark:
    hadoop:
      # Spark SQL Hive settings
      spark.sql.warehouse.dir: "s3a://${MINIO_BUCKET}/warehouse"
      spark.sql.catalogImplementation: "hive"
      # Direct PostgreSQL connection for Hive metastore
      spark.hadoop.javax.jdo.option.ConnectionURL: "jdbc:postgresql://${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}"
      spark.hadoop.javax.jdo.option.ConnectionDriverName: "org.postgresql.Driver"
      spark.hadoop.javax.jdo.option.ConnectionUserName: "${POSTGRES_USER}"
      spark.hadoop.javax.jdo.option.ConnectionPassword: "${POSTGRES_PASSWORD}"
      # Disable schema verification and SASL for simplicity
      spark.hadoop.datanucleus.schema.autoCreateAll: "true"
      spark.hadoop.datanucleus.autoCreateSchema: "true"
      spark.hadoop.datanucleus.fixedDatastore: "false"
      spark.hadoop.datanucleus.autoCreateTables: "true"

# Delta Lake Configuration
delta:
  enabled: true
  spark:
    hadoop:
      # Delta Lake specific settings
      spark.sql.extensions: "io.delta.sql.DeltaSparkSessionExtension"
      spark.sql.catalog.spark_catalog: "org.apache.spark.sql.delta.catalog.DeltaCatalog"
      # Delta Lake table properties
      spark.delta.logStore.class: "org.apache.spark.sql.delta.storage.S3SingleDriverLogStore"
      spark.delta.merge.repartitionBeforeWrite: "true"
      spark.delta.autoOptimize.optimizeWrite: "true"
      spark.delta.autoOptimize.autoCompact: "true"
      # Delta Lake S3 settings
      spark.delta.storage.s3.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
      spark.delta.storage.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
      # Delta Lake table paths
      spark.delta.warehouse.dir: "s3a://${MINIO_BUCKET}/delta"
      # Delta Lake performance settings
      spark.delta.optimizeWrite.enabled: "true"
      spark.delta.autoCompact.enabled: "true"
      spark.delta.optimizeWrite.numShuffleBlocks: "200"
      spark.delta.optimizeWrite.targetFileSize: "128m"
      # Delta Lake concurrent writes settings
      spark.delta.concurrent.writes.enabled: "true"
      spark.delta.concurrent.writes.maxConcurrentWrites: "10"
      # Delta Lake schema evolution
      spark.delta.schema.autoMerge.enabled: "true"
      # Delta Lake time travel settings
      spark.delta.timeTravel.enabled: "true"
      spark.delta.timeTravel.retentionPeriod: "168h" # 7 days

# PostgreSQL configuration
postgres:
  host: "${POSTGRES_HOST}"
  port: "${POSTGRES_PORT}"
  database: "${POSTGRES_DB:-hive}"
  # These values will be stored in the spark-arm-postgres secret
  user: "${POSTGRES_USER}"
  password: "${POSTGRES_PASSWORD}"
